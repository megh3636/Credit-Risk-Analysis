{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Randomly choose 20% of observations from \"train_labels.csv\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "train_labels = pd.read_csv('train_labels.csv')\n",
    "\n",
    "# 20% sample of train_labels.csv\n",
    "sample_train_labels = train_labels.sample(frac=0.2, random_state=1)\n",
    "sample_train_lables.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge this sample with “train_data.csv”**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20% sample of train_data.csv\n",
    "sample_train_data = pd.merge(sample_train_labels, train_data,on='customer_ID',how='inner')\n",
    "sample_train_data.head()\n",
    "\n",
    "sample_train_data.to_csv('sample_train_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explore the data. Data Size, data type of features, a snapshot of data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: (1106, 191)\n",
      "Data Types:\n",
      "customer_ID     object\n",
      "target           int64\n",
      "S_2             object\n",
      "P_2            float64\n",
      "D_39           float64\n",
      "                ...   \n",
      "D_141          float64\n",
      "D_142          float64\n",
      "D_143          float64\n",
      "D_144          float64\n",
      "D_145          float64\n",
      "Length: 191, dtype: object\n",
      "                                         customer_ID  target         S_2  \\\n",
      "0  19f2288c04bc71bbf67fbdc9bbfb539803c8d628845f9b...       0  2017-08-08   \n",
      "1  afb2974efdf3abbe7f13f110d66de91d62f84e777f2908...       0  2018-03-16   \n",
      "2  019bfdcdd3618bb6c2ee4ee27e212bd8fdb6ee4ee8c4b4...       0  2017-12-20   \n",
      "3  fb6c8e102405adf305c505a54e665a47d0b713f56a4915...       1  2017-11-26   \n",
      "4  20ebe898ee384d68a424ef7dd86c9be1ce3ed181e0f491...       0  2017-10-30   \n",
      "\n",
      "        P_2      D_39       B_1       B_2       R_1       S_3      D_41  ...  \\\n",
      "0  0.952106  0.006830  0.007750  0.817398  0.000341       NaN  0.002549  ...   \n",
      "1  0.838285  0.003378  0.009451  0.818851  0.001573       NaN  0.002360  ...   \n",
      "2  0.850569  0.302643  0.018813  1.008960  0.000978  0.132243  0.002743  ...   \n",
      "3  0.446003  0.000986  0.114735  0.076269  0.001983  0.350972  0.001733  ...   \n",
      "4  0.651073  0.622973  0.130193  0.266888  0.518292  0.165631  0.002417  ...   \n",
      "\n",
      "   D_136  D_137  D_138     D_139     D_140     D_141     D_142     D_143  \\\n",
      "0    NaN    NaN    NaN  1.005263  0.000440  1.046311  0.892046  1.008361   \n",
      "1    NaN    NaN    NaN  1.006692  0.001486  0.884677  0.299482  1.008416   \n",
      "2    NaN    NaN    NaN  0.000541  0.009812  0.006955       NaN  0.002825   \n",
      "3    NaN    NaN    NaN  0.006414  0.007076  0.005997       NaN  0.003399   \n",
      "4    NaN    NaN    NaN  0.004322  0.007612  0.004255       NaN  0.006625   \n",
      "\n",
      "      D_144     D_145  \n",
      "0  1.003621  0.094888  \n",
      "1  0.008062  0.552013  \n",
      "2  0.008781  0.007286  \n",
      "3  0.003733  0.003048  \n",
      "4  0.002431  0.003079  \n",
      "\n",
      "[5 rows x 191 columns]\n",
      "Snapshot:\n",
      "                                         customer_ID  target         S_2  \\\n",
      "0  19f2288c04bc71bbf67fbdc9bbfb539803c8d628845f9b...       0  2017-08-08   \n",
      "1  afb2974efdf3abbe7f13f110d66de91d62f84e777f2908...       0  2018-03-16   \n",
      "2  019bfdcdd3618bb6c2ee4ee27e212bd8fdb6ee4ee8c4b4...       0  2017-12-20   \n",
      "3  fb6c8e102405adf305c505a54e665a47d0b713f56a4915...       1  2017-11-26   \n",
      "4  20ebe898ee384d68a424ef7dd86c9be1ce3ed181e0f491...       0  2017-10-30   \n",
      "\n",
      "        P_2      D_39       B_1       B_2       R_1       S_3      D_41  ...  \\\n",
      "0  0.952106  0.006830  0.007750  0.817398  0.000341       NaN  0.002549  ...   \n",
      "1  0.838285  0.003378  0.009451  0.818851  0.001573       NaN  0.002360  ...   \n",
      "2  0.850569  0.302643  0.018813  1.008960  0.000978  0.132243  0.002743  ...   \n",
      "3  0.446003  0.000986  0.114735  0.076269  0.001983  0.350972  0.001733  ...   \n",
      "4  0.651073  0.622973  0.130193  0.266888  0.518292  0.165631  0.002417  ...   \n",
      "\n",
      "   D_136  D_137  D_138     D_139     D_140     D_141     D_142     D_143  \\\n",
      "0    NaN    NaN    NaN  1.005263  0.000440  1.046311  0.892046  1.008361   \n",
      "1    NaN    NaN    NaN  1.006692  0.001486  0.884677  0.299482  1.008416   \n",
      "2    NaN    NaN    NaN  0.000541  0.009812  0.006955       NaN  0.002825   \n",
      "3    NaN    NaN    NaN  0.006414  0.007076  0.005997       NaN  0.003399   \n",
      "4    NaN    NaN    NaN  0.004322  0.007612  0.004255       NaN  0.006625   \n",
      "\n",
      "      D_144     D_145  \n",
      "0  1.003621  0.094888  \n",
      "1  0.008062  0.552013  \n",
      "2  0.008781  0.007286  \n",
      "3  0.003733  0.003048  \n",
      "4  0.002431  0.003079  \n",
      "\n",
      "[5 rows x 191 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('sample_train_data.csv')\n",
    "\n",
    "# Data size\n",
    "print(f\"Data Size: {df.shape}\")\n",
    "\n",
    "# data type of features\n",
    "print(f\"Data Types:\\n{df.dtypes}\")\n",
    "\n",
    "#head\n",
    "print(df.head())\n",
    "print(f\"Snapshot:\\n{df.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'S_2' column to datetime\n",
    "df['S_2'] = pd.to_datetime(df['S_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2017-08-08\n",
       "1   2018-03-16\n",
       "2   2017-12-20\n",
       "3   2017-11-26\n",
       "4   2017-10-30\n",
       "Name: S_2, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['S_2'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imputing the missing values**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Number of misssing values*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values Count: \n",
      " customer_ID      0\n",
      "target           0\n",
      "S_2              0\n",
      "P_2              7\n",
      "D_39             0\n",
      "              ... \n",
      "D_141           18\n",
      "D_142          916\n",
      "D_143           18\n",
      "D_144            6\n",
      "D_145           18\n",
      "Length: 191, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing Values Count: \\n\", missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Replace numerical columns with mean and categorical columns with mode*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
    "\n",
    "for column in categorical_columns:\n",
    "    df[column] = df[column].fillna(df[column].mode()[0])\n",
    "    \n",
    "numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "for column in numerical_columns:\n",
    "    df[column] = df[column].fillna(df[column].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values Count: \n",
      " customer_ID    0\n",
      "target         0\n",
      "S_2            0\n",
      "P_2            0\n",
      "D_39           0\n",
      "              ..\n",
      "D_141          0\n",
      "D_142          0\n",
      "D_143          0\n",
      "D_144          0\n",
      "D_145          0\n",
      "Length: 191, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing Values Count: \\n\", missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-Hot Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m columns_to_encode \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB_30\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB_38\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD_114\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD_116\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD_117\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD_120\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD_126\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD_63\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD_64\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD_66\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD_68\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m df_encoded \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(data, columns\u001b[38;5;241m=\u001b[39mcolumns_to_encode, prefix\u001b[38;5;241m=\u001b[39mcolumns_to_encode)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "columns_to_encode = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
    "df_encoded = pd.get_dummies(data, columns=columns_to_encode, prefix=columns_to_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.to_csv('df_encoded.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>target</th>\n",
       "      <th>S_2</th>\n",
       "      <th>P_2</th>\n",
       "      <th>D_39</th>\n",
       "      <th>B_1</th>\n",
       "      <th>B_2</th>\n",
       "      <th>R_1</th>\n",
       "      <th>S_3</th>\n",
       "      <th>D_41</th>\n",
       "      <th>...</th>\n",
       "      <th>D_64_U</th>\n",
       "      <th>D_66_0.0</th>\n",
       "      <th>D_66_1.0</th>\n",
       "      <th>D_68_0.0</th>\n",
       "      <th>D_68_1.0</th>\n",
       "      <th>D_68_2.0</th>\n",
       "      <th>D_68_3.0</th>\n",
       "      <th>D_68_4.0</th>\n",
       "      <th>D_68_5.0</th>\n",
       "      <th>D_68_6.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19f2288c04bc71bbf67fbdc9bbfb539803c8d628845f9b...</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-08-08</td>\n",
       "      <td>0.952106</td>\n",
       "      <td>0.006830</td>\n",
       "      <td>0.007750</td>\n",
       "      <td>0.817398</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.217330</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>afb2974efdf3abbe7f13f110d66de91d62f84e777f2908...</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-03-16</td>\n",
       "      <td>0.838285</td>\n",
       "      <td>0.003378</td>\n",
       "      <td>0.009451</td>\n",
       "      <td>0.818851</td>\n",
       "      <td>0.001573</td>\n",
       "      <td>0.217330</td>\n",
       "      <td>0.002360</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>019bfdcdd3618bb6c2ee4ee27e212bd8fdb6ee4ee8c4b4...</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-12-20</td>\n",
       "      <td>0.850569</td>\n",
       "      <td>0.302643</td>\n",
       "      <td>0.018813</td>\n",
       "      <td>1.008960</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>0.132243</td>\n",
       "      <td>0.002743</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fb6c8e102405adf305c505a54e665a47d0b713f56a4915...</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-11-26</td>\n",
       "      <td>0.446003</td>\n",
       "      <td>0.000986</td>\n",
       "      <td>0.114735</td>\n",
       "      <td>0.076269</td>\n",
       "      <td>0.001983</td>\n",
       "      <td>0.350972</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20ebe898ee384d68a424ef7dd86c9be1ce3ed181e0f491...</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-10-30</td>\n",
       "      <td>0.651073</td>\n",
       "      <td>0.622973</td>\n",
       "      <td>0.130193</td>\n",
       "      <td>0.266888</td>\n",
       "      <td>0.518292</td>\n",
       "      <td>0.165631</td>\n",
       "      <td>0.002417</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 224 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_ID  target         S_2  \\\n",
       "0  19f2288c04bc71bbf67fbdc9bbfb539803c8d628845f9b...       0  2017-08-08   \n",
       "1  afb2974efdf3abbe7f13f110d66de91d62f84e777f2908...       0  2018-03-16   \n",
       "2  019bfdcdd3618bb6c2ee4ee27e212bd8fdb6ee4ee8c4b4...       0  2017-12-20   \n",
       "3  fb6c8e102405adf305c505a54e665a47d0b713f56a4915...       1  2017-11-26   \n",
       "4  20ebe898ee384d68a424ef7dd86c9be1ce3ed181e0f491...       0  2017-10-30   \n",
       "\n",
       "        P_2      D_39       B_1       B_2       R_1       S_3      D_41  ...  \\\n",
       "0  0.952106  0.006830  0.007750  0.817398  0.000341  0.217330  0.002549  ...   \n",
       "1  0.838285  0.003378  0.009451  0.818851  0.001573  0.217330  0.002360  ...   \n",
       "2  0.850569  0.302643  0.018813  1.008960  0.000978  0.132243  0.002743  ...   \n",
       "3  0.446003  0.000986  0.114735  0.076269  0.001983  0.350972  0.001733  ...   \n",
       "4  0.651073  0.622973  0.130193  0.266888  0.518292  0.165631  0.002417  ...   \n",
       "\n",
       "   D_64_U  D_66_0.0  D_66_1.0  D_68_0.0  D_68_1.0  D_68_2.0  D_68_3.0  \\\n",
       "0       0         0         1         0         0         0         0   \n",
       "1       0         0         1         0         0         0         0   \n",
       "2       0         0         1         0         0         0         0   \n",
       "3       1         0         1         0         0         0         0   \n",
       "4       0         0         1         0         0         0         0   \n",
       "\n",
       "   D_68_4.0  D_68_5.0  D_68_6.0  \n",
       "0         0         0         1  \n",
       "1         0         1         0  \n",
       "2         0         0         1  \n",
       "3         0         1         0  \n",
       "4         0         0         1  \n",
       "\n",
       "[5 rows x 224 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('df_encoded.csv')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['customer_ID', 'target', 'S_2', 'P_2', 'D_39', 'B_1', 'B_2', 'R_1',\n",
      "       'S_3', 'D_41',\n",
      "       ...\n",
      "       'D_64_U', 'D_66_0.0', 'D_66_1.0', 'D_68_0.0', 'D_68_1.0', 'D_68_2.0',\n",
      "       'D_68_3.0', 'D_68_4.0', 'D_68_5.0', 'D_68_6.0'],\n",
      "      dtype='object', length=224)\n"
     ]
    }
   ],
   "source": [
    "print(df2.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining new features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame 'df' with columns D_*, S_*, P_*, B_*, and R_*\n",
    "# Replace 'df' with your actual DataFrame name\n",
    "\n",
    "# Delinquency Average (D_Avg) excluding specified columns\n",
    "delinquency_columns = [col for col in df.columns if col.startswith('D_') and col not in ['D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']]\n",
    "df2['D_Avg'] = df[delinquency_columns].mean(axis=1)\n",
    "\n",
    "# Spend Average (S_Avg) excluding S_2\n",
    "spend_columns = [col for col in df.columns if col.startswith('S_') and col != 'S_2']\n",
    "df2['S_Avg'] = df[spend_columns].mean(axis=1)\n",
    "\n",
    "# Payment Average (P_Avg)\n",
    "payment_columns = [col for col in df.columns if col.startswith('P_')]\n",
    "df2['P_Avg'] = df[payment_columns].mean(axis=1)\n",
    "\n",
    "# Balance Average (B_Avg) excluding specified columns\n",
    "balance_columns = [col for col in df.columns if col.startswith('B_') and col not in ['B_30', 'B_38']]\n",
    "df2['B_Avg'] = df[balance_columns].mean(axis=1)\n",
    "\n",
    "# Risk Average (R_Avg)\n",
    "risk_columns = [col for col in df.columns if col.startswith('R_')]\n",
    "df2['R_Avg'] = df[risk_columns].mean(axis=1)\n",
    "\n",
    "# Now, 'D_Avg', 'S_Avg', 'P_Avg', 'B_Avg', and 'R_Avg' are added as new columns to your DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            customer_ID  target         S_2  \\\n",
      "0     19f2288c04bc71bbf67fbdc9bbfb539803c8d628845f9b...       0  2017-08-08   \n",
      "1     afb2974efdf3abbe7f13f110d66de91d62f84e777f2908...       0  2018-03-16   \n",
      "2     019bfdcdd3618bb6c2ee4ee27e212bd8fdb6ee4ee8c4b4...       0  2017-12-20   \n",
      "3     fb6c8e102405adf305c505a54e665a47d0b713f56a4915...       1  2017-11-26   \n",
      "4     20ebe898ee384d68a424ef7dd86c9be1ce3ed181e0f491...       0  2017-10-30   \n",
      "...                                                 ...     ...         ...   \n",
      "1101  982106b8b7c2628d4f09a882643661a202b4bcf8bf4cfa...       1  2017-07-14   \n",
      "1102  40a9928cea9880d517398ec6fdc736e7535fe6663f02b3...       0  2018-03-31   \n",
      "1103  71248f965b357a76ff05e962914abbdb7d3c0cb94cf2b5...       0  2017-07-10   \n",
      "1104  94853b18179ff80a65b253c92b0e4275db1d2f63bce074...       0  2017-07-22   \n",
      "1105  784b2970a958963d0d3a4b40fb2a84b778c80ef63316da...       0  2018-01-02   \n",
      "\n",
      "           P_2      D_39       B_1       B_2       R_1       S_3      D_41  \\\n",
      "0     0.952106  0.006830  0.007750  0.817398  0.000341  0.217330  0.002549   \n",
      "1     0.838285  0.003378  0.009451  0.818851  0.001573  0.217330  0.002360   \n",
      "2     0.850569  0.302643  0.018813  1.008960  0.000978  0.132243  0.002743   \n",
      "3     0.446003  0.000986  0.114735  0.076269  0.001983  0.350972  0.001733   \n",
      "4     0.651073  0.622973  0.130193  0.266888  0.518292  0.165631  0.002417   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1101  0.319138  0.715330  0.115719  0.044145  0.504295  0.483726  0.006918   \n",
      "1102  0.510783  0.353735  0.047859  1.000810  0.007733  0.043515  0.009784   \n",
      "1103  0.975702  0.001245  0.012763  0.993044  0.005897  0.160437  0.004931   \n",
      "1104  0.917180  0.009148  0.003619  0.844280  0.009020  0.297457  0.003553   \n",
      "1105  0.562659  0.442086  0.112207  0.814789  0.006449  0.315005  0.004953   \n",
      "\n",
      "      ...  D_68_2.0  D_68_3.0  D_68_4.0  D_68_5.0  D_68_6.0     D_Avg  \\\n",
      "0     ...         0         0         0         0         1  0.220955   \n",
      "1     ...         0         0         0         1         0  0.200995   \n",
      "2     ...         0         0         0         0         1  0.167951   \n",
      "3     ...         0         0         0         1         0  0.222348   \n",
      "4     ...         0         0         0         0         1  0.272485   \n",
      "...   ...       ...       ...       ...       ...       ...       ...   \n",
      "1101  ...         0         0         0         1         0  0.242563   \n",
      "1102  ...         0         0         0         0         1  0.214322   \n",
      "1103  ...         0         0         0         1         0  0.193896   \n",
      "1104  ...         0         0         0         1         0  0.240975   \n",
      "1105  ...         0         0         0         0         1  0.276294   \n",
      "\n",
      "         S_Avg     P_Avg     B_Avg     R_Avg  \n",
      "0     0.205051  0.559213  0.135581  0.086062  \n",
      "1     0.204653  0.515581  0.169696  0.085890  \n",
      "2     0.210438  0.495196  0.152239  0.085483  \n",
      "3     0.253713  0.637075  0.161539  0.100723  \n",
      "4     0.346197  0.383251  0.194320  0.129447  \n",
      "...        ...       ...       ...       ...  \n",
      "1101  0.376399  0.262694  0.227321  0.111909  \n",
      "1102  0.294170  0.398095  0.196545  0.060291  \n",
      "1103  0.217031  0.527963  0.138111  0.086428  \n",
      "1104  0.218317  0.507875  0.127531  0.086548  \n",
      "1105  0.345129  0.368484  0.239741  0.085418  \n",
      "\n",
      "[1106 rows x 229 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['customer_ID', 'target', 'S_2', 'P_2', 'D_39', 'B_1', 'B_2', 'R_1',\n",
      "       'S_3', 'D_41',\n",
      "       ...\n",
      "       'D_68_2.0', 'D_68_3.0', 'D_68_4.0', 'D_68_5.0', 'D_68_6.0', 'D_Avg',\n",
      "       'S_Avg', 'P_Avg', 'B_Avg', 'R_Avg'],\n",
      "      dtype='object', length=229)\n"
     ]
    }
   ],
   "source": [
    "print(df2.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (774, 229)\n",
      "Test1 set shape: (166, 229)\n",
      "Test2 set shape: (166, 229)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 70% train set and a 30% test set\n",
    "train_set, test_set = train_test_split(df2, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split test set into a 15% test1 set and a 15% test2 set\n",
    "test1_set, test2_set = train_test_split(test_set, test_size=0.5, random_state=42)\n",
    "\n",
    "#resulting sets\n",
    "print(\"Train set shape:\", train_set.shape)\n",
    "print(\"Test1 set shape:\", test1_set.shape)\n",
    "print(\"Test2 set shape:\", test2_set.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\anaconda\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: numpy in c:\\anaconda\\lib\\site-packages (from xgboost) (1.23.5)\n",
      "Requirement already satisfied: scipy in c:\\anaconda\\lib\\site-packages (from xgboost) (1.10.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split data into features (X) and target (y)\n",
    "X = train_set.drop(columns=['target','S_2','customer_ID'])\n",
    "y = train_set['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost model with default parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# missing values\n",
    "X_train.fillna(np.nan, inplace=True)\n",
    "X_test.fillna(np.nan, inplace=True)\n",
    "\n",
    "# XGBClassifier object\n",
    "xg_clf = xgb.XGBClassifier(use_label_encoder=False)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "xg_clf.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Feature  Importance\n",
      "0           P_2    0.085065\n",
      "1          D_39    0.000508\n",
      "2           B_1    0.001408\n",
      "3           B_2    0.001490\n",
      "4           R_1    0.007729\n",
      "5           S_3    0.004153\n",
      "6          D_41    0.012401\n",
      "7           B_3    0.024768\n",
      "8          D_42    0.024512\n",
      "9          D_43    0.016045\n",
      "10         D_44    0.002668\n",
      "11          B_4    0.000000\n",
      "12         D_45    0.000000\n",
      "13          B_5    0.003968\n",
      "14          R_2    0.007054\n",
      "15         D_46    0.014268\n",
      "16         D_47    0.015507\n",
      "17         D_48    0.034329\n",
      "18         D_49    0.000000\n",
      "19          B_6    0.000000\n",
      "20          B_7    0.000000\n",
      "21          B_8    0.000709\n",
      "22         D_50    0.000000\n",
      "23         D_51    0.002868\n",
      "24          B_9    0.003120\n",
      "25          R_3    0.011194\n",
      "26         D_52    0.006036\n",
      "27          P_3    0.000651\n",
      "28         B_10    0.000000\n",
      "29         D_53    0.007052\n",
      "30          S_5    0.022897\n",
      "31         B_11    0.000000\n",
      "32          S_6    0.003901\n",
      "33         D_54    0.002919\n",
      "34          R_4    0.000000\n",
      "35          S_7    0.002000\n",
      "36         B_12    0.004758\n",
      "37          S_8    0.017171\n",
      "38         D_55    0.001606\n",
      "39         D_56    0.000000\n",
      "40         B_13    0.000242\n",
      "41          R_5    0.000999\n",
      "42         D_58    0.004501\n",
      "43          S_9    0.006014\n",
      "44         B_14    0.000946\n",
      "45         D_59    0.000000\n",
      "46         D_60    0.000000\n",
      "47         D_61    0.008680\n",
      "48         B_15    0.028007\n",
      "49         S_11    0.005642\n",
      "50         D_62    0.002541\n",
      "51         D_65    0.004664\n",
      "52         B_16    0.011734\n",
      "53         B_17    0.002987\n",
      "54         B_18    0.037997\n",
      "55         B_19    0.019354\n",
      "56         B_20    0.009336\n",
      "57         S_12    0.008244\n",
      "58          R_6    0.018296\n",
      "59         S_13    0.016669\n",
      "60         B_21    0.000000\n",
      "61         D_69    0.002420\n",
      "62         B_22    0.005791\n",
      "63         D_70    0.013361\n",
      "64         D_71    0.000000\n",
      "65         D_72    0.000852\n",
      "66         S_15    0.000722\n",
      "67         B_23    0.012012\n",
      "68         D_73    0.000000\n",
      "69          P_4    0.004304\n",
      "70         D_74    0.000000\n",
      "71         D_75    0.000000\n",
      "72         D_76    0.000000\n",
      "73         B_24    0.005194\n",
      "74          R_7    0.004380\n",
      "75         D_77    0.013711\n",
      "76         B_25    0.000000\n",
      "77         B_26    0.000997\n",
      "78         D_78    0.006584\n",
      "79         D_79    0.005428\n",
      "80          R_8    0.006455\n",
      "81          R_9    0.000000\n",
      "82         S_16    0.001565\n",
      "83         D_80    0.001618\n",
      "84         R_10    0.000591\n",
      "85         R_11    0.003019\n",
      "86         B_27    0.006293\n",
      "87         D_81    0.000254\n",
      "88         D_82    0.001493\n",
      "89         S_17    0.014922\n",
      "90         R_12    0.008763\n",
      "91         B_28    0.001571\n",
      "92         R_13    0.006959\n",
      "93         D_83    0.004452\n",
      "94         R_14    0.005900\n",
      "95         R_15    0.002151\n",
      "96         D_84    0.009415\n",
      "97         R_16    0.003318\n",
      "98         B_29    0.000000\n",
      "99         S_18    0.012784\n",
      "100        D_86    0.005004\n",
      "101        D_87    0.000000\n",
      "102        R_17    0.004634\n",
      "103        R_18    0.000566\n",
      "104        D_88    0.000000\n",
      "105        B_31    0.000000\n",
      "106        S_19    0.003291\n",
      "107        R_19    0.002786\n",
      "108        B_32    0.000764\n",
      "109        S_20    0.001485\n",
      "110        R_20    0.026141\n",
      "111        R_21    0.010186\n",
      "112        B_33    0.000811\n",
      "113        D_89    0.001237\n",
      "114        R_22    0.001584\n",
      "115        R_23    0.000849\n",
      "116        D_91    0.002809\n",
      "117        D_92    0.001035\n",
      "118        D_93    0.003079\n",
      "119        D_94    0.006905\n",
      "120        R_24    0.001079\n",
      "121        R_25    0.001633\n",
      "122        D_96    0.014001\n",
      "123        S_22    0.002561\n",
      "124        S_23    0.003583\n",
      "125        S_24    0.001151\n",
      "126        S_25    0.001846\n",
      "127        S_26    0.000871\n",
      "128       D_102    0.012064\n",
      "129       D_103    0.004301\n",
      "130       D_104    0.000643\n",
      "131       D_105    0.000000\n",
      "132       D_106    0.000000\n",
      "133       D_107    0.013014\n",
      "134        B_36    0.000000\n",
      "135        B_37    0.000996\n",
      "136        R_26    0.000000\n",
      "137        R_27    0.001895\n",
      "138       D_108    0.000000\n",
      "139       D_109    0.008070\n",
      "140       D_110    0.000000\n",
      "141       D_111    0.000000\n",
      "142        B_39    0.000000\n",
      "143       D_112    0.004010\n",
      "144        B_40    0.005989\n",
      "145        S_27    0.013378\n",
      "146       D_113    0.009938\n",
      "147       D_115    0.000000\n",
      "148       D_118    0.004459\n",
      "149       D_119    0.000000\n",
      "150       D_121    0.003660\n",
      "151       D_122    0.004024\n",
      "152       D_123    0.005639\n",
      "153       D_124    0.001130\n",
      "154       D_125    0.003003\n",
      "155       D_127    0.001380\n",
      "156       D_128    0.007122\n",
      "157       D_129    0.004013\n",
      "158        B_41    0.000576\n",
      "159        B_42    0.000000\n",
      "160       D_130    0.006429\n",
      "161       D_131    0.000810\n",
      "162       D_132    0.000000\n",
      "163       D_133    0.000000\n",
      "164        R_28    0.007524\n",
      "165       D_134    0.000000\n",
      "166       D_135    0.000000\n",
      "167       D_136    0.000000\n",
      "168       D_137    0.000000\n",
      "169       D_138    0.000000\n",
      "170       D_139    0.001279\n",
      "171       D_140    0.001976\n",
      "172       D_141    0.005314\n",
      "173       D_142    0.000000\n",
      "174       D_143    0.000000\n",
      "175       D_144    0.000531\n",
      "176       D_145    0.000977\n",
      "177    B_30_0.0    0.000000\n",
      "178    B_30_1.0    0.000000\n",
      "179    B_30_2.0    0.000000\n",
      "180    B_38_1.0    0.000000\n",
      "181    B_38_2.0    0.000000\n",
      "182    B_38_3.0    0.000415\n",
      "183    B_38_4.0    0.000000\n",
      "184    B_38_5.0    0.000000\n",
      "185    B_38_6.0    0.000000\n",
      "186    B_38_7.0    0.000000\n",
      "187   D_114_0.0    0.000000\n",
      "188   D_114_1.0    0.000000\n",
      "189   D_116_0.0    0.000000\n",
      "190   D_116_1.0    0.000000\n",
      "191  D_117_-1.0    0.000000\n",
      "192   D_117_1.0    0.000000\n",
      "193   D_117_2.0    0.000000\n",
      "194   D_117_3.0    0.000000\n",
      "195   D_117_4.0    0.000000\n",
      "196   D_117_5.0    0.000000\n",
      "197   D_117_6.0    0.000000\n",
      "198   D_120_0.0    0.007749\n",
      "199   D_120_1.0    0.000000\n",
      "200  D_126_-1.0    0.000000\n",
      "201   D_126_0.0    0.000000\n",
      "202   D_126_1.0    0.025387\n",
      "203     D_63_CL    0.000000\n",
      "204     D_63_CO    0.000000\n",
      "205     D_63_CR    0.000000\n",
      "206     D_63_XM    0.000000\n",
      "207     D_63_XZ    0.000000\n",
      "208     D_64_-1    0.000000\n",
      "209      D_64_O    0.000000\n",
      "210      D_64_R    0.000000\n",
      "211      D_64_U    0.000000\n",
      "212    D_66_0.0    0.000000\n",
      "213    D_66_1.0    0.000000\n",
      "214    D_68_0.0    0.000000\n",
      "215    D_68_1.0    0.000000\n",
      "216    D_68_2.0    0.000000\n",
      "217    D_68_3.0    0.015545\n",
      "218    D_68_4.0    0.000000\n",
      "219    D_68_5.0    0.000000\n",
      "220    D_68_6.0    0.000000\n",
      "221       D_Avg    0.002472\n",
      "222       S_Avg    0.008629\n",
      "223       P_Avg    0.000710\n",
      "224       B_Avg    0.002142\n",
      "225       R_Avg    0.006033\n"
     ]
    }
   ],
   "source": [
    "feature_importances = xg_clf.feature_importances_\n",
    "features_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "features_df.to_csv('feature_importances.csv', index=False)\n",
    "print (features_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost model with given parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.5, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.5, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=300, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.5, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.5, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=300, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.5, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.5, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=4, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=300, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "#weights for the target variable\n",
    "weights = compute_sample_weight(class_weight={0:1, 1:5}, y=y_train)\n",
    "\n",
    "# Create an XGBClassifier object\n",
    "xg_clf2 = xgb.XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.5,\n",
    "    max_depth=4,\n",
    "    subsample=0.5,\n",
    "    colsample_bytree=0.5,\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "xg_clf2.fit(X_train, y_train, sample_weight=weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Feature  Importance\n",
      "0           P_2    0.064741\n",
      "1          D_39    0.000000\n",
      "2           B_1    0.000000\n",
      "3           B_2    0.000434\n",
      "4           R_1    0.001168\n",
      "5           S_3    0.019812\n",
      "6          D_41    0.002237\n",
      "7           B_3    0.006155\n",
      "8          D_42    0.021527\n",
      "9          D_43    0.014676\n",
      "10         D_44    0.004504\n",
      "11          B_4    0.000277\n",
      "12         D_45    0.000371\n",
      "13          B_5    0.002855\n",
      "14          R_2    0.012541\n",
      "15         D_46    0.001664\n",
      "16         D_47    0.010839\n",
      "17         D_48    0.039507\n",
      "18         D_49    0.000000\n",
      "19          B_6    0.000000\n",
      "20          B_7    0.002495\n",
      "21          B_8    0.001840\n",
      "22         D_50    0.000000\n",
      "23         D_51    0.008647\n",
      "24          B_9    0.008646\n",
      "25          R_3    0.003398\n",
      "26         D_52    0.008377\n",
      "27          P_3    0.004544\n",
      "28         B_10    0.000118\n",
      "29         D_53    0.017204\n",
      "30          S_5    0.020963\n",
      "31         B_11    0.000000\n",
      "32          S_6    0.001938\n",
      "33         D_54    0.006925\n",
      "34          R_4    0.000000\n",
      "35          S_7    0.015076\n",
      "36         B_12    0.002116\n",
      "37          S_8    0.000386\n",
      "38         D_55    0.025396\n",
      "39         D_56    0.018514\n",
      "40         B_13    0.006903\n",
      "41          R_5    0.001474\n",
      "42         D_58    0.000152\n",
      "43          S_9    0.003190\n",
      "44         B_14    0.000754\n",
      "45         D_59    0.000197\n",
      "46         D_60    0.007598\n",
      "47         D_61    0.002146\n",
      "48         B_15    0.000537\n",
      "49         S_11    0.008326\n",
      "50         D_62    0.000000\n",
      "51         D_65    0.000152\n",
      "52         B_16    0.005603\n",
      "53         B_17    0.000332\n",
      "54         B_18    0.000820\n",
      "55         B_19    0.003211\n",
      "56         B_20    0.029689\n",
      "57         S_12    0.000758\n",
      "58          R_6    0.011090\n",
      "59         S_13    0.002493\n",
      "60         B_21    0.009242\n",
      "61         D_69    0.005248\n",
      "62         B_22    0.008573\n",
      "63         D_70    0.000785\n",
      "64         D_71    0.000507\n",
      "65         D_72    0.004982\n",
      "66         S_15    0.004498\n",
      "67         B_23    0.000000\n",
      "68         D_73    0.000000\n",
      "69          P_4    0.007914\n",
      "70         D_74    0.000000\n",
      "71         D_75    0.000246\n",
      "72         D_76    0.000000\n",
      "73         B_24    0.007279\n",
      "74          R_7    0.000329\n",
      "75         D_77    0.010981\n",
      "76         B_25    0.024790\n",
      "77         B_26    0.002004\n",
      "78         D_78    0.014421\n",
      "79         D_79    0.006531\n",
      "80          R_8    0.010706\n",
      "81          R_9    0.000000\n",
      "82         S_16    0.007446\n",
      "83         D_80    0.002064\n",
      "84         R_10    0.000632\n",
      "85         R_11    0.001489\n",
      "86         B_27    0.003543\n",
      "87         D_81    0.014551\n",
      "88         D_82    0.000000\n",
      "89         S_17    0.000677\n",
      "90         R_12    0.015969\n",
      "91         B_28    0.000423\n",
      "92         R_13    0.003628\n",
      "93         D_83    0.012196\n",
      "94         R_14    0.002855\n",
      "95         R_15    0.003782\n",
      "96         D_84    0.003311\n",
      "97         R_16    0.006126\n",
      "98         B_29    0.000000\n",
      "99         S_18    0.001613\n",
      "100        D_86    0.001293\n",
      "101        D_87    0.000000\n",
      "102        R_17    0.007191\n",
      "103        R_18    0.004385\n",
      "104        D_88    0.000000\n",
      "105        B_31    0.000000\n",
      "106        S_19    0.000000\n",
      "107        R_19    0.000497\n",
      "108        B_32    0.001211\n",
      "109        S_20    0.003322\n",
      "110        R_20    0.006583\n",
      "111        R_21    0.015153\n",
      "112        B_33    0.002688\n",
      "113        D_89    0.003047\n",
      "114        R_22    0.000899\n",
      "115        R_23    0.005175\n",
      "116        D_91    0.006002\n",
      "117        D_92    0.002326\n",
      "118        D_93    0.005917\n",
      "119        D_94    0.000000\n",
      "120        R_24    0.000906\n",
      "121        R_25    0.004392\n",
      "122        D_96    0.000920\n",
      "123        S_22    0.010461\n",
      "124        S_23    0.000378\n",
      "125        S_24    0.007422\n",
      "126        S_25    0.005176\n",
      "127        S_26    0.000000\n",
      "128       D_102    0.001199\n",
      "129       D_103    0.024997\n",
      "130       D_104    0.000942\n",
      "131       D_105    0.001160\n",
      "132       D_106    0.000000\n",
      "133       D_107    0.001174\n",
      "134        B_36    0.000334\n",
      "135        B_37    0.000719\n",
      "136        R_26    0.012272\n",
      "137        R_27    0.000203\n",
      "138       D_108    0.000000\n",
      "139       D_109    0.004512\n",
      "140       D_110    0.000000\n",
      "141       D_111    0.000000\n",
      "142        B_39    0.000000\n",
      "143       D_112    0.015626\n",
      "144        B_40    0.013605\n",
      "145        S_27    0.001220\n",
      "146       D_113    0.017120\n",
      "147       D_115    0.004188\n",
      "148       D_118    0.001548\n",
      "149       D_119    0.005809\n",
      "150       D_121    0.000000\n",
      "151       D_122    0.003466\n",
      "152       D_123    0.007440\n",
      "153       D_124    0.008137\n",
      "154       D_125    0.001925\n",
      "155       D_127    0.001434\n",
      "156       D_128    0.002597\n",
      "157       D_129    0.016689\n",
      "158        B_41    0.000771\n",
      "159        B_42    0.000000\n",
      "160       D_130    0.000000\n",
      "161       D_131    0.009332\n",
      "162       D_132    0.000000\n",
      "163       D_133    0.004222\n",
      "164        R_28    0.018025\n",
      "165       D_134    0.000000\n",
      "166       D_135    0.000000\n",
      "167       D_136    0.000000\n",
      "168       D_137    0.000000\n",
      "169       D_138    0.000000\n",
      "170       D_139    0.008120\n",
      "171       D_140    0.000896\n",
      "172       D_141    0.003558\n",
      "173       D_142    0.005946\n",
      "174       D_143    0.000325\n",
      "175       D_144    0.008929\n",
      "176       D_145    0.002756\n",
      "177    B_30_0.0    0.000000\n",
      "178    B_30_1.0    0.000000\n",
      "179    B_30_2.0    0.000000\n",
      "180    B_38_1.0    0.000000\n",
      "181    B_38_2.0    0.000000\n",
      "182    B_38_3.0    0.000000\n",
      "183    B_38_4.0    0.000000\n",
      "184    B_38_5.0    0.000000\n",
      "185    B_38_6.0    0.000000\n",
      "186    B_38_7.0    0.000000\n",
      "187   D_114_0.0    0.000000\n",
      "188   D_114_1.0    0.000000\n",
      "189   D_116_0.0    0.000000\n",
      "190   D_116_1.0    0.000000\n",
      "191  D_117_-1.0    0.000000\n",
      "192   D_117_1.0    0.000000\n",
      "193   D_117_2.0    0.000000\n",
      "194   D_117_3.0    0.000000\n",
      "195   D_117_4.0    0.000000\n",
      "196   D_117_5.0    0.000000\n",
      "197   D_117_6.0    0.000000\n",
      "198   D_120_0.0    0.000000\n",
      "199   D_120_1.0    0.000000\n",
      "200  D_126_-1.0    0.000000\n",
      "201   D_126_0.0    0.000000\n",
      "202   D_126_1.0    0.006618\n",
      "203     D_63_CL    0.000000\n",
      "204     D_63_CO    0.000000\n",
      "205     D_63_CR    0.000000\n",
      "206     D_63_XM    0.000000\n",
      "207     D_63_XZ    0.000000\n",
      "208     D_64_-1    0.000000\n",
      "209      D_64_O    0.000000\n",
      "210      D_64_R    0.000000\n",
      "211      D_64_U    0.000000\n",
      "212    D_66_0.0    0.000000\n",
      "213    D_66_1.0    0.000000\n",
      "214    D_68_0.0    0.000000\n",
      "215    D_68_1.0    0.000000\n",
      "216    D_68_2.0    0.000000\n",
      "217    D_68_3.0    0.000000\n",
      "218    D_68_4.0    0.000000\n",
      "219    D_68_5.0    0.000000\n",
      "220    D_68_6.0    0.000119\n",
      "221       D_Avg    0.000290\n",
      "222       S_Avg    0.010131\n",
      "223       P_Avg    0.014452\n",
      "224       B_Avg    0.014719\n",
      "225       R_Avg    0.012475\n"
     ]
    }
   ],
   "source": [
    "# feature importance\n",
    "feature_importances2 = xg_clf2.feature_importances_\n",
    "\n",
    "features_importances2 = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances2\n",
    "})\n",
    "\n",
    "features_importances2.to_csv('feature_importances2.csv', index=False)\n",
    "print (features_importances2.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Feature       Min  1st Percentile  5th Percentile    Median  \\\n",
      "0       P_2 -0.136627        0.014908        0.236069  0.695273   \n",
      "17     D_48 -0.004654        0.002627        0.016767  0.374200   \n",
      "56     B_20  0.000002        0.000187        0.000888  0.008372   \n",
      "38     D_55  0.000248        0.002215        0.008273  0.190922   \n",
      "129   D_103  0.000006        0.000239        0.001096  0.009900   \n",
      "\n",
      "     95th Percentile  99th Percentile       Max      Mean  % Missing  \n",
      "0           0.966178         1.004166  1.009898  0.658426        0.0  \n",
      "17          0.933638         0.984735  1.134253  0.374411        0.0  \n",
      "56          1.006336         1.008939  1.009892  0.232326        0.0  \n",
      "38          0.871996         1.026036  1.436484  0.297460        0.0  \n",
      "129         1.008824         1.009770  1.009989  0.490416        0.0  \n"
     ]
    }
   ],
   "source": [
    "# Sort features_importances2 by 'Importance' in descending order\n",
    "sorted_features_importances2 = features_importances2.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Get the top 5 features\n",
    "top_5_features = sorted_features_importances2.head(5)\n",
    "\n",
    "# Calculate the requested statistics for the top 5 features\n",
    "statistics = {\n",
    "    'Feature': top_5_features['Feature'],\n",
    "    'Min': [],\n",
    "    '1st Percentile': [],\n",
    "    '5th Percentile': [],\n",
    "    'Median': [],\n",
    "    '95th Percentile': [],\n",
    "    '99th Percentile': [],\n",
    "    'Max': [],\n",
    "    'Mean': [],\n",
    "    '% Missing': []\n",
    "}\n",
    "\n",
    "for feature in top_5_features['Feature']:\n",
    "    # Calculate statistics for the feature\n",
    "    min_value = df2[feature].min()\n",
    "    percentile_1 = df2[feature].quantile(0.01)\n",
    "    percentile_5 = df2[feature].quantile(0.05)\n",
    "    median = df2[feature].median()\n",
    "    percentile_95 = df2[feature].quantile(0.95)\n",
    "    percentile_99 = df2[feature].quantile(0.99)\n",
    "    max_value = df2[feature].max()\n",
    "    mean = df2[feature].mean()\n",
    "    missing_percentage = (df2[feature].isnull().mean()) * 100\n",
    "\n",
    "    # Append values to the statistics dictionary\n",
    "    statistics['Min'].append(min_value)\n",
    "    statistics['1st Percentile'].append(percentile_1)\n",
    "    statistics['5th Percentile'].append(percentile_5)\n",
    "    statistics['Median'].append(median)\n",
    "    statistics['95th Percentile'].append(percentile_95)\n",
    "    statistics['99th Percentile'].append(percentile_99)\n",
    "    statistics['Max'].append(max_value)\n",
    "    statistics['Mean'].append(mean)\n",
    "    statistics['% Missing'].append(missing_percentage)\n",
    "\n",
    "# Create a DataFrame from the statistics dictionary\n",
    "top_5_feature_statistics = pd.DataFrame(statistics)\n",
    "\n",
    "# Print the statistics for the top 5 features\n",
    "print(top_5_feature_statistics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features with feature importance of higher 0.5% in any of the two models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_default_model = pd.read_csv('feature_importances.csv')\n",
    "features_parameters_model = pd.read_csv('feature_importances2.csv')\n",
    "\n",
    "threshold = 0.5 / 100  \n",
    "\n",
    "#features that meet the threshold in both models\n",
    "features_default_model_above_threshold = features_default_model[features_default_model['Importance'] > threshold]['Feature']\n",
    "features_parameters_model_above_threshold = features_parameters_model[features_parameters_model['Importance'] > threshold]['Feature']\n",
    "\n",
    "# Combining the features from both models\n",
    "selected_features_df = pd.concat([features_default_model_above_threshold, features_parameters_model_above_threshold]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "selected_features_df.to_csv('selected_features_df.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = selected_features_df.tolist()  # (converting pandas series to list)selected\n",
    "df3 = df2[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P_2</th>\n",
       "      <th>R_1</th>\n",
       "      <th>D_41</th>\n",
       "      <th>B_3</th>\n",
       "      <th>D_42</th>\n",
       "      <th>D_43</th>\n",
       "      <th>R_2</th>\n",
       "      <th>D_46</th>\n",
       "      <th>D_47</th>\n",
       "      <th>D_48</th>\n",
       "      <th>...</th>\n",
       "      <th>D_112</th>\n",
       "      <th>D_119</th>\n",
       "      <th>D_124</th>\n",
       "      <th>D_129</th>\n",
       "      <th>D_131</th>\n",
       "      <th>D_139</th>\n",
       "      <th>D_142</th>\n",
       "      <th>D_144</th>\n",
       "      <th>P_Avg</th>\n",
       "      <th>B_Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.952106</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.004538</td>\n",
       "      <td>0.192096</td>\n",
       "      <td>0.070317</td>\n",
       "      <td>0.008067</td>\n",
       "      <td>0.476357</td>\n",
       "      <td>0.489047</td>\n",
       "      <td>0.374411</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000786</td>\n",
       "      <td>0.629443</td>\n",
       "      <td>0.145341</td>\n",
       "      <td>0.008729</td>\n",
       "      <td>0.009824</td>\n",
       "      <td>1.005263</td>\n",
       "      <td>0.892046</td>\n",
       "      <td>1.003621</td>\n",
       "      <td>0.559213</td>\n",
       "      <td>0.135581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.838285</td>\n",
       "      <td>0.001573</td>\n",
       "      <td>0.002360</td>\n",
       "      <td>0.002085</td>\n",
       "      <td>0.192096</td>\n",
       "      <td>0.041391</td>\n",
       "      <td>0.005375</td>\n",
       "      <td>0.476357</td>\n",
       "      <td>0.111964</td>\n",
       "      <td>0.124475</td>\n",
       "      <td>...</td>\n",
       "      <td>1.006283</td>\n",
       "      <td>0.160460</td>\n",
       "      <td>0.006584</td>\n",
       "      <td>0.005382</td>\n",
       "      <td>0.004034</td>\n",
       "      <td>1.006692</td>\n",
       "      <td>0.299482</td>\n",
       "      <td>0.008062</td>\n",
       "      <td>0.515581</td>\n",
       "      <td>0.169696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.850569</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>0.002743</td>\n",
       "      <td>0.003825</td>\n",
       "      <td>0.192096</td>\n",
       "      <td>0.151929</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.440366</td>\n",
       "      <td>0.386611</td>\n",
       "      <td>0.013712</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000643</td>\n",
       "      <td>0.798512</td>\n",
       "      <td>0.006228</td>\n",
       "      <td>0.007306</td>\n",
       "      <td>0.005804</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>0.414144</td>\n",
       "      <td>0.008781</td>\n",
       "      <td>0.495196</td>\n",
       "      <td>0.152239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.446003</td>\n",
       "      <td>0.001983</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>0.126152</td>\n",
       "      <td>0.218584</td>\n",
       "      <td>0.092902</td>\n",
       "      <td>0.001781</td>\n",
       "      <td>0.487935</td>\n",
       "      <td>0.182341</td>\n",
       "      <td>0.575451</td>\n",
       "      <td>...</td>\n",
       "      <td>1.002467</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.414803</td>\n",
       "      <td>0.006453</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.006414</td>\n",
       "      <td>0.414144</td>\n",
       "      <td>0.003733</td>\n",
       "      <td>0.637075</td>\n",
       "      <td>0.161539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.651073</td>\n",
       "      <td>0.518292</td>\n",
       "      <td>0.002417</td>\n",
       "      <td>0.117436</td>\n",
       "      <td>0.192096</td>\n",
       "      <td>0.205960</td>\n",
       "      <td>0.008466</td>\n",
       "      <td>0.506387</td>\n",
       "      <td>0.379591</td>\n",
       "      <td>0.603235</td>\n",
       "      <td>...</td>\n",
       "      <td>1.009632</td>\n",
       "      <td>0.331499</td>\n",
       "      <td>0.323869</td>\n",
       "      <td>1.006943</td>\n",
       "      <td>0.978709</td>\n",
       "      <td>0.004322</td>\n",
       "      <td>0.414144</td>\n",
       "      <td>0.002431</td>\n",
       "      <td>0.383251</td>\n",
       "      <td>0.194320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        P_2       R_1      D_41       B_3      D_42      D_43       R_2  \\\n",
       "0  0.952106  0.000341  0.002549  0.004538  0.192096  0.070317  0.008067   \n",
       "1  0.838285  0.001573  0.002360  0.002085  0.192096  0.041391  0.005375   \n",
       "2  0.850569  0.000978  0.002743  0.003825  0.192096  0.151929  0.006100   \n",
       "3  0.446003  0.001983  0.001733  0.126152  0.218584  0.092902  0.001781   \n",
       "4  0.651073  0.518292  0.002417  0.117436  0.192096  0.205960  0.008466   \n",
       "\n",
       "       D_46      D_47      D_48  ...     D_112     D_119     D_124     D_129  \\\n",
       "0  0.476357  0.489047  0.374411  ...  1.000786  0.629443  0.145341  0.008729   \n",
       "1  0.476357  0.111964  0.124475  ...  1.006283  0.160460  0.006584  0.005382   \n",
       "2  0.440366  0.386611  0.013712  ...  1.000643  0.798512  0.006228  0.007306   \n",
       "3  0.487935  0.182341  0.575451  ...  1.002467  0.004400  0.414803  0.006453   \n",
       "4  0.506387  0.379591  0.603235  ...  1.009632  0.331499  0.323869  1.006943   \n",
       "\n",
       "      D_131     D_139     D_142     D_144     P_Avg     B_Avg  \n",
       "0  0.009824  1.005263  0.892046  1.003621  0.559213  0.135581  \n",
       "1  0.004034  1.006692  0.299482  0.008062  0.515581  0.169696  \n",
       "2  0.005804  0.000541  0.414144  0.008781  0.495196  0.152239  \n",
       "3  0.000624  0.006414  0.414144  0.003733  0.637075  0.161539  \n",
       "4  0.978709  0.004322  0.414144  0.002431  0.383251  0.194320  \n",
       "\n",
       "[5 rows x 98 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grid Search for the XGBoost model (using only chosen features) with given parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Setup*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "X = df3\n",
    "y = df2['target']\n",
    "X_train, X_test1, y_train, y_test1 = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_test1, X_test2, y_test1, y_test2 = train_test_split(X_test1, y_test1, test_size=0.5, random_state=42)\n",
    "\n",
    "# Grid parameters\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 300],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'subsample': [0.5, 0.8],\n",
    "    'colsample_bytree': [0.5, 1.0],\n",
    "    'scale_pos_weight': [1, 5, 10]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fit the model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, device=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False, eval_metric=None,\n",
       "                                     feature_types=None, gamma=None,\n",
       "                                     grow_policy=None, importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None,...\n",
       "                                     max_delta_step=None, max_depth=None,\n",
       "                                     max_leaves=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     multi_strategy=None, n_estimators=None,\n",
       "                                     n_jobs=None, num_parallel_tree=None,\n",
       "                                     random_state=None, ...),\n",
       "             param_grid={&#x27;colsample_bytree&#x27;: [0.5, 1.0],\n",
       "                         &#x27;learning_rate&#x27;: [0.01, 0.1],\n",
       "                         &#x27;n_estimators&#x27;: [50, 100, 300],\n",
       "                         &#x27;scale_pos_weight&#x27;: [1, 5, 10],\n",
       "                         &#x27;subsample&#x27;: [0.5, 0.8]},\n",
       "             scoring=&#x27;roc_auc&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, device=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False, eval_metric=None,\n",
       "                                     feature_types=None, gamma=None,\n",
       "                                     grow_policy=None, importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None,...\n",
       "                                     max_delta_step=None, max_depth=None,\n",
       "                                     max_leaves=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     multi_strategy=None, n_estimators=None,\n",
       "                                     n_jobs=None, num_parallel_tree=None,\n",
       "                                     random_state=None, ...),\n",
       "             param_grid={&#x27;colsample_bytree&#x27;: [0.5, 1.0],\n",
       "                         &#x27;learning_rate&#x27;: [0.01, 0.1],\n",
       "                         &#x27;n_estimators&#x27;: [50, 100, 300],\n",
       "                         &#x27;scale_pos_weight&#x27;: [1, 5, 10],\n",
       "                         &#x27;subsample&#x27;: [0.5, 0.8]},\n",
       "             scoring=&#x27;roc_auc&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, device=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False, eval_metric=None,\n",
       "                                     feature_types=None, gamma=None,\n",
       "                                     grow_policy=None, importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None,...\n",
       "                                     max_delta_step=None, max_depth=None,\n",
       "                                     max_leaves=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     multi_strategy=None, n_estimators=None,\n",
       "                                     n_jobs=None, num_parallel_tree=None,\n",
       "                                     random_state=None, ...),\n",
       "             param_grid={'colsample_bytree': [0.5, 1.0],\n",
       "                         'learning_rate': [0.01, 0.1],\n",
       "                         'n_estimators': [50, 100, 300],\n",
       "                         'scale_pos_weight': [1, 5, 10],\n",
       "                         'subsample': [0.5, 0.8]},\n",
       "             scoring='roc_auc')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGBClassifier object\n",
    "xg_clf3 = XGBClassifier(use_label_encoder=False)\n",
    "\n",
    "# GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=xg_clf3, param_grid=param_grid, cv=3, scoring='roc_auc')\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Best Parameters from the GridSearchCV object*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 1.0,\n",
       " 'learning_rate': 0.01,\n",
       " 'n_estimators': 300,\n",
       " 'scale_pos_weight': 1,\n",
       " 'subsample': 0.5}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = grid_search.best_params_\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Prepare table*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Trees</th>\n",
       "      <th>LR</th>\n",
       "      <th>Subsample %</th>\n",
       "      <th>Features</th>\n",
       "      <th>% Weight of Default</th>\n",
       "      <th>AUC Train</th>\n",
       "      <th>AUC Test 1</th>\n",
       "      <th>AUC Test 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>50.0%</td>\n",
       "      <td>50.0%</td>\n",
       "      <td>1</td>\n",
       "      <td>0.983845</td>\n",
       "      <td>0.878442</td>\n",
       "      <td>0.915079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>80.0%</td>\n",
       "      <td>50.0%</td>\n",
       "      <td>1</td>\n",
       "      <td>0.994216</td>\n",
       "      <td>0.884964</td>\n",
       "      <td>0.916468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>50.0%</td>\n",
       "      <td>50.0%</td>\n",
       "      <td>5</td>\n",
       "      <td>0.987458</td>\n",
       "      <td>0.882971</td>\n",
       "      <td>0.907738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>80.0%</td>\n",
       "      <td>50.0%</td>\n",
       "      <td>5</td>\n",
       "      <td>0.996687</td>\n",
       "      <td>0.888768</td>\n",
       "      <td>0.914286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>50.0%</td>\n",
       "      <td>50.0%</td>\n",
       "      <td>10</td>\n",
       "      <td>0.979833</td>\n",
       "      <td>0.891123</td>\n",
       "      <td>0.908730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>300</td>\n",
       "      <td>0.10</td>\n",
       "      <td>80.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884058</td>\n",
       "      <td>0.930357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>300</td>\n",
       "      <td>0.10</td>\n",
       "      <td>50.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.893116</td>\n",
       "      <td>0.915476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>300</td>\n",
       "      <td>0.10</td>\n",
       "      <td>80.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884058</td>\n",
       "      <td>0.924603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>300</td>\n",
       "      <td>0.10</td>\n",
       "      <td>50.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.889674</td>\n",
       "      <td>0.913492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>300</td>\n",
       "      <td>0.10</td>\n",
       "      <td>80.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.882971</td>\n",
       "      <td>0.925992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   # Trees    LR Subsample % Features % Weight of Default  AUC Train  \\\n",
       "0       50  0.01       50.0%    50.0%                   1   0.983845   \n",
       "1       50  0.01       80.0%    50.0%                   1   0.994216   \n",
       "2       50  0.01       50.0%    50.0%                   5   0.987458   \n",
       "3       50  0.01       80.0%    50.0%                   5   0.996687   \n",
       "4       50  0.01       50.0%    50.0%                  10   0.979833   \n",
       "..     ...   ...         ...      ...                 ...        ...   \n",
       "67     300  0.10       80.0%   100.0%                   1   1.000000   \n",
       "68     300  0.10       50.0%   100.0%                   5   1.000000   \n",
       "69     300  0.10       80.0%   100.0%                   5   1.000000   \n",
       "70     300  0.10       50.0%   100.0%                  10   1.000000   \n",
       "71     300  0.10       80.0%   100.0%                  10   1.000000   \n",
       "\n",
       "    AUC Test 1  AUC Test 2  \n",
       "0     0.878442    0.915079  \n",
       "1     0.884964    0.916468  \n",
       "2     0.882971    0.907738  \n",
       "3     0.888768    0.914286  \n",
       "4     0.891123    0.908730  \n",
       "..         ...         ...  \n",
       "67    0.884058    0.930357  \n",
       "68    0.893116    0.915476  \n",
       "69    0.884058    0.924603  \n",
       "70    0.889674    0.913492  \n",
       "71    0.882971    0.925992  \n",
       "\n",
       "[72 rows x 8 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Create an empty DataFrame\n",
    "results_df = pd.DataFrame(columns=['# Trees', 'LR', 'Subsample %', 'Features', '% Weight of Default', 'AUC Train', 'AUC Test 1', 'AUC Test 2'])\n",
    "\n",
    "# Iterate over each combination of settings\n",
    "for i, params in enumerate(grid_search.cv_results_['params']):\n",
    "    \n",
    "    # Fit model with current parameters\n",
    "    xg_clf3.set_params(**params)\n",
    "    xg_clf3.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict probabilities for each set\n",
    "    train_pred = xg_clf3.predict_proba(X_train)[:, 1]\n",
    "    test1_pred = xg_clf3.predict_proba(X_test1)[:, 1]\n",
    "    test2_pred = xg_clf3.predict_proba(X_test2)[:, 1]\n",
    "    \n",
    "    # Calculate AUC for each set\n",
    "    auc_train = roc_auc_score(y_train, train_pred)\n",
    "    auc_test1 = roc_auc_score(y_test1, test1_pred)\n",
    "    auc_test2 = roc_auc_score(y_test2, test2_pred)\n",
    "    \n",
    "    # Create a temporary DataFrame\n",
    "    temp_df = pd.DataFrame({\n",
    "        '# Trees': params['n_estimators'],\n",
    "        'LR': params['learning_rate'],\n",
    "        'Subsample %': [f'{params[\"subsample\"] * 100}%'],\n",
    "        'Features': [f'{params[\"colsample_bytree\"] * 100}%'],\n",
    "        '% Weight of Default': [params['scale_pos_weight']],\n",
    "        'AUC Train': [auc_train],\n",
    "        'AUC Test 1': [auc_test1],\n",
    "        'AUC Test 2': [auc_test2]\n",
    "    })\n",
    "    \n",
    "    # Concatenate the temporary DataFrame with the results DataFrame\n",
    "    results_df = pd.concat([results_df, temp_df], ignore_index=True)\n",
    "\n",
    "results_df.to_csv('grid_search_XGBoost.csv', index=False)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract AUC Test 1 and its standard deviation\n",
    "auc_test1 = results_df['AUC Test 1']\n",
    "std_auc_test1 = results_df['AUC Test 1'].std()\n",
    "\n",
    "# Create an array of the same length as x_values with the standard deviation value\n",
    "std_values = np.full_like(auc_test1, std_auc_test1)\n",
    "\n",
    "# Plot a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(auc_test1, std_values, marker='o')\n",
    "plt.xlabel('AUC Test 1')\n",
    "plt.ylabel('Standard Deviation of AUC Test 1')\n",
    "plt.title('AUC Test 1 vs. Standard Deviation of AUC Test 1')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose the best model, based on bias and variance ; Re-run the model with optimum parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Parameters:\n",
      "# Trees                      50\n",
      "LR                          0.1\n",
      "Subsample %               80.0%\n",
      "Features                  50.0%\n",
      "% Weight of Default           1\n",
      "AUC Train                   1.0\n",
      "AUC Test 1             0.911594\n",
      "AUC Test 2             0.922222\n",
      "Name: 19, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Find the row with the highest average AUC\n",
    "best_model_row = results_df.loc[results_df['AUC Test 1'].idxmax()]\n",
    "\n",
    "best_model_params = {\n",
    "    'n_estimators': best_model_row['# Trees'],\n",
    "    'learning_rate': best_model_row['LR'],\n",
    "    'subsample': float(best_model_row['Subsample %'][:-1]) / 100.0,\n",
    "    'colsample_bytree': float(best_model_row['Features'][:-1]) / 100.0,\n",
    "    'scale_pos_weight': best_model_row['% Weight of Default']\n",
    "}\n",
    "\n",
    "# Print information about the best model\n",
    "print(\"Best Model Parameters:\")\n",
    "print(best_model_row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Re-run the model with optimum parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.5, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=50, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.5, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=50, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.5, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=50, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_best_model = XGBClassifier(**best_model_params)\n",
    "xg_best_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Save to file*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_best_model.save_model('xgb_best_model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**#Data Processing (We will use only features that we chose in step 10)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P_2</th>\n",
       "      <th>R_1</th>\n",
       "      <th>D_41</th>\n",
       "      <th>B_3</th>\n",
       "      <th>D_42</th>\n",
       "      <th>D_43</th>\n",
       "      <th>R_2</th>\n",
       "      <th>D_46</th>\n",
       "      <th>D_47</th>\n",
       "      <th>D_48</th>\n",
       "      <th>...</th>\n",
       "      <th>D_112</th>\n",
       "      <th>D_119</th>\n",
       "      <th>D_124</th>\n",
       "      <th>D_129</th>\n",
       "      <th>D_131</th>\n",
       "      <th>D_139</th>\n",
       "      <th>D_142</th>\n",
       "      <th>D_144</th>\n",
       "      <th>P_Avg</th>\n",
       "      <th>B_Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.230267</td>\n",
       "      <td>-0.343392</td>\n",
       "      <td>-0.317740</td>\n",
       "      <td>-0.551914</td>\n",
       "      <td>0.025955</td>\n",
       "      <td>-0.587999</td>\n",
       "      <td>-0.204683</td>\n",
       "      <td>0.002469</td>\n",
       "      <td>0.323218</td>\n",
       "      <td>0.001939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.398993</td>\n",
       "      <td>1.472233</td>\n",
       "      <td>-0.759960</td>\n",
       "      <td>-0.891894</td>\n",
       "      <td>-0.310068</td>\n",
       "      <td>2.191870</td>\n",
       "      <td>3.847772</td>\n",
       "      <td>5.419180</td>\n",
       "      <td>0.668549</td>\n",
       "      <td>-1.002463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.752592</td>\n",
       "      <td>-0.337519</td>\n",
       "      <td>-0.319050</td>\n",
       "      <td>-0.562145</td>\n",
       "      <td>0.025955</td>\n",
       "      <td>-0.804500</td>\n",
       "      <td>-0.217519</td>\n",
       "      <td>0.002469</td>\n",
       "      <td>-1.241372</td>\n",
       "      <td>-0.817533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414682</td>\n",
       "      <td>-0.476584</td>\n",
       "      <td>-1.386304</td>\n",
       "      <td>-0.898690</td>\n",
       "      <td>-0.330295</td>\n",
       "      <td>2.195661</td>\n",
       "      <td>-1.183604</td>\n",
       "      <td>-0.241488</td>\n",
       "      <td>0.343031</td>\n",
       "      <td>-0.486007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.804143</td>\n",
       "      <td>-0.340354</td>\n",
       "      <td>-0.316400</td>\n",
       "      <td>-0.554886</td>\n",
       "      <td>0.025955</td>\n",
       "      <td>0.022837</td>\n",
       "      <td>-0.214059</td>\n",
       "      <td>-0.338593</td>\n",
       "      <td>-0.101807</td>\n",
       "      <td>-1.180694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.398585</td>\n",
       "      <td>2.174785</td>\n",
       "      <td>-1.387909</td>\n",
       "      <td>-0.894784</td>\n",
       "      <td>-0.324113</td>\n",
       "      <td>-0.474828</td>\n",
       "      <td>0.016009</td>\n",
       "      <td>-0.237402</td>\n",
       "      <td>0.190944</td>\n",
       "      <td>-0.750287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.893711</td>\n",
       "      <td>-0.335568</td>\n",
       "      <td>-0.323388</td>\n",
       "      <td>-0.044635</td>\n",
       "      <td>0.453459</td>\n",
       "      <td>-0.418962</td>\n",
       "      <td>-0.234655</td>\n",
       "      <td>0.112180</td>\n",
       "      <td>-0.949363</td>\n",
       "      <td>0.661095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.403791</td>\n",
       "      <td>-1.125078</td>\n",
       "      <td>0.456379</td>\n",
       "      <td>-0.896516</td>\n",
       "      <td>-0.342210</td>\n",
       "      <td>-0.459242</td>\n",
       "      <td>0.016009</td>\n",
       "      <td>-0.266103</td>\n",
       "      <td>1.249447</td>\n",
       "      <td>-0.609492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.033087</td>\n",
       "      <td>2.123792</td>\n",
       "      <td>-0.318654</td>\n",
       "      <td>-0.080994</td>\n",
       "      <td>0.025955</td>\n",
       "      <td>0.427234</td>\n",
       "      <td>-0.202778</td>\n",
       "      <td>0.287035</td>\n",
       "      <td>-0.130934</td>\n",
       "      <td>0.752189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.424239</td>\n",
       "      <td>0.234152</td>\n",
       "      <td>0.045908</td>\n",
       "      <td>1.134707</td>\n",
       "      <td>3.074985</td>\n",
       "      <td>-0.464792</td>\n",
       "      <td>0.016009</td>\n",
       "      <td>-0.273509</td>\n",
       "      <td>-0.644228</td>\n",
       "      <td>-0.113235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        P_2       R_1      D_41       B_3      D_42      D_43       R_2  \\\n",
       "0  1.230267 -0.343392 -0.317740 -0.551914  0.025955 -0.587999 -0.204683   \n",
       "1  0.752592 -0.337519 -0.319050 -0.562145  0.025955 -0.804500 -0.217519   \n",
       "2  0.804143 -0.340354 -0.316400 -0.554886  0.025955  0.022837 -0.214059   \n",
       "3 -0.893711 -0.335568 -0.323388 -0.044635  0.453459 -0.418962 -0.234655   \n",
       "4 -0.033087  2.123792 -0.318654 -0.080994  0.025955  0.427234 -0.202778   \n",
       "\n",
       "       D_46      D_47      D_48  ...     D_112     D_119     D_124     D_129  \\\n",
       "0  0.002469  0.323218  0.001939  ...  0.398993  1.472233 -0.759960 -0.891894   \n",
       "1  0.002469 -1.241372 -0.817533  ...  0.414682 -0.476584 -1.386304 -0.898690   \n",
       "2 -0.338593 -0.101807 -1.180694  ...  0.398585  2.174785 -1.387909 -0.894784   \n",
       "3  0.112180 -0.949363  0.661095  ...  0.403791 -1.125078  0.456379 -0.896516   \n",
       "4  0.287035 -0.130934  0.752189  ...  0.424239  0.234152  0.045908  1.134707   \n",
       "\n",
       "      D_131     D_139     D_142     D_144     P_Avg     B_Avg  \n",
       "0 -0.310068  2.191870  3.847772  5.419180  0.668549 -1.002463  \n",
       "1 -0.330295  2.195661 -1.183604 -0.241488  0.343031 -0.486007  \n",
       "2 -0.324113 -0.474828  0.016009 -0.237402  0.190944 -0.750287  \n",
       "3 -0.342210 -0.459242  0.016009 -0.266103  1.249447 -0.609492  \n",
       "4  3.074985 -0.464792  0.016009 -0.273509 -0.644228 -0.113235  \n",
       "\n",
       "[5 rows x 98 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Find the 1st and 99th percentiles for outlier treatment\n",
    "Q1 = df3.quantile(0.01)\n",
    "Q99 = df3.quantile(0.99)\n",
    "\n",
    "# Cap and floor the values based on the 1st and 99th percentiles\n",
    "df3 = df3.clip(Q1, Q99, axis=1)\n",
    "\n",
    "# Replace missing values with 0\n",
    "df3.fillna(0, inplace=True)\n",
    "\n",
    "# Apply StandardScaler to normalize the data\n",
    "scaler = StandardScaler()\n",
    "df3_scaled = scaler.fit_transform(df3)\n",
    "\n",
    "# Convert the numpy array back to a DataFrame\n",
    "df3_scaled = pd.DataFrame(df3_scaled, columns=selected_features)\n",
    "\n",
    "# Display the first few rows of the processed DataFrame\n",
    "df3_scaled.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df3_scaled\n",
    "y = df2['target']\n",
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "Model with 2 layers, 4 nodes per layer, relu activation, 50% dropout, and batch size 100 finished training. Train AUC: 0.8978521766086532, Test AUC 1: 0.8728260869565218, Test AUC 2: 0.9115079365079364\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "Model with 2 layers, 4 nodes per layer, relu activation, 50% dropout, and batch size 10000 finished training. Train AUC: 0.6805411629245826, Test AUC 1: 0.41286231884057967, Test AUC 2: 0.2910714285714286\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "Model with 2 layers, 4 nodes per layer, relu activation, 0% dropout, and batch size 100 finished training. Train AUC: 0.9329347681679288, Test AUC 1: 0.8692028985507246, Test AUC 2: 0.9001984126984127\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "Model with 2 layers, 4 nodes per layer, relu activation, 0% dropout, and batch size 10000 finished training. Train AUC: 0.7792214693769097, Test AUC 1: 0.5204710144927537, Test AUC 2: 0.5333333333333333\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "Model with 2 layers, 4 nodes per layer, tanh activation, 50% dropout, and batch size 100 finished training. Train AUC: 0.9341216066604667, Test AUC 1: 0.8878623188405798, Test AUC 2: 0.8958333333333334\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "Model with 2 layers, 4 nodes per layer, tanh activation, 50% dropout, and batch size 10000 finished training. Train AUC: 0.7754218148000531, Test AUC 1: 0.7539855072463768, Test AUC 2: 0.7321428571428571\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "Model with 2 layers, 4 nodes per layer, tanh activation, 0% dropout, and batch size 100 finished training. Train AUC: 0.9431380364022851, Test AUC 1: 0.8778985507246377, Test AUC 2: 0.9113095238095239\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "Model with 2 layers, 4 nodes per layer, tanh activation, 0% dropout, and batch size 10000 finished training. Train AUC: 0.5917187015632611, Test AUC 1: 0.621195652173913, Test AUC 2: 0.641468253968254\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "Model with 2 layers, 6 nodes per layer, relu activation, 50% dropout, and batch size 100 finished training. Train AUC: 0.9179664319560693, Test AUC 1: 0.8556159420289855, Test AUC 2: 0.8869047619047619\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "Model with 2 layers, 6 nodes per layer, relu activation, 50% dropout, and batch size 10000 finished training. Train AUC: 0.811939240954785, Test AUC 1: 0.7028985507246377, Test AUC 2: 0.789484126984127\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "Model with 2 layers, 6 nodes per layer, relu activation, 0% dropout, and batch size 100 finished training. Train AUC: 0.9309330853372304, Test AUC 1: 0.8621376811594202, Test AUC 2: 0.9092261904761904\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "Model with 2 layers, 6 nodes per layer, relu activation, 0% dropout, and batch size 10000 finished training. Train AUC: 0.8036490855143705, Test AUC 1: 0.6934782608695652, Test AUC 2: 0.7529761904761906\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "Model with 2 layers, 6 nodes per layer, tanh activation, 50% dropout, and batch size 100 finished training. Train AUC: 0.9404809352995883, Test AUC 1: 0.8681159420289856, Test AUC 2: 0.9095238095238095\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "Model with 2 layers, 6 nodes per layer, tanh activation, 50% dropout, and batch size 10000 finished training. Train AUC: 0.7488419467694079, Test AUC 1: 0.7784420289855073, Test AUC 2: 0.8087301587301587\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "Model with 2 layers, 6 nodes per layer, tanh activation, 0% dropout, and batch size 100 finished training. Train AUC: 0.939683804968779, Test AUC 1: 0.8952898550724637, Test AUC 2: 0.923611111111111\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "Model with 2 layers, 6 nodes per layer, tanh activation, 0% dropout, and batch size 10000 finished training. Train AUC: 0.7841902484389531, Test AUC 1: 0.7961956521739131, Test AUC 2: 0.7787698412698413\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "Model with 4 layers, 4 nodes per layer, relu activation, 50% dropout, and batch size 100 finished training. Train AUC: 0.7053540587219342, Test AUC 1: 0.7797101449275362, Test AUC 2: 0.8155753968253968\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "Model with 4 layers, 4 nodes per layer, relu activation, 50% dropout, and batch size 10000 finished training. Train AUC: 0.4969709047429255, Test AUC 1: 0.5797101449275361, Test AUC 2: 0.6117063492063493\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "Model with 4 layers, 4 nodes per layer, relu activation, 0% dropout, and batch size 100 finished training. Train AUC: 0.9168415924892609, Test AUC 1: 0.8782608695652174, Test AUC 2: 0.8884920634920634\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "Model with 4 layers, 4 nodes per layer, relu activation, 0% dropout, and batch size 10000 finished training. Train AUC: 0.452473318276427, Test AUC 1: 0.37472826086956523, Test AUC 2: 0.4726190476190476\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "Model with 4 layers, 4 nodes per layer, tanh activation, 50% dropout, and batch size 100 finished training. Train AUC: 0.9018112572516718, Test AUC 1: 0.8807971014492754, Test AUC 2: 0.8869047619047619\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "Model with 4 layers, 4 nodes per layer, tanh activation, 50% dropout, and batch size 10000 finished training. Train AUC: 0.6405296488198042, Test AUC 1: 0.6981884057971015, Test AUC 2: 0.6426587301587301\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "Model with 4 layers, 4 nodes per layer, tanh activation, 0% dropout, and batch size 100 finished training. Train AUC: 0.9415880607590452, Test AUC 1: 0.8576086956521739, Test AUC 2: 0.9093253968253968\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "Model with 4 layers, 4 nodes per layer, tanh activation, 0% dropout, and batch size 10000 finished training. Train AUC: 0.8216730879943315, Test AUC 1: 0.8235507246376812, Test AUC 2: 0.8994047619047619\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "Model with 4 layers, 6 nodes per layer, relu activation, 50% dropout, and batch size 100 finished training. Train AUC: 0.9156591824985606, Test AUC 1: 0.8541666666666667, Test AUC 2: 0.8901785714285715\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "Model with 4 layers, 6 nodes per layer, relu activation, 50% dropout, and batch size 10000 finished training. Train AUC: 0.5774146406270758, Test AUC 1: 0.4628623188405797, Test AUC 2: 0.40476190476190477\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "Model with 4 layers, 6 nodes per layer, relu activation, 0% dropout, and batch size 100 finished training. Train AUC: 0.9403392232407776, Test AUC 1: 0.8487318840579711, Test AUC 2: 0.8863095238095239\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "Model with 4 layers, 6 nodes per layer, relu activation, 0% dropout, and batch size 10000 finished training. Train AUC: 0.7994951507904875, Test AUC 1: 0.7650362318840579, Test AUC 2: 0.7317460317460318\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "Model with 4 layers, 6 nodes per layer, tanh activation, 50% dropout, and batch size 100 finished training. Train AUC: 0.9103316947876532, Test AUC 1: 0.8048913043478261, Test AUC 2: 0.8966269841269842\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "Model with 4 layers, 6 nodes per layer, tanh activation, 50% dropout, and batch size 10000 finished training. Train AUC: 0.7867056374828395, Test AUC 1: 0.7376811594202898, Test AUC 2: 0.7966269841269842\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "Model with 4 layers, 6 nodes per layer, tanh activation, 0% dropout, and batch size 100 finished training. Train AUC: 0.9504539214383774, Test AUC 1: 0.8822463768115942, Test AUC 2: 0.9109126984126984\n",
      "25/25 [==============================] - 0s 3ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "Model with 4 layers, 6 nodes per layer, tanh activation, 0% dropout, and batch size 10000 finished training. Train AUC: 0.7662282449847216, Test AUC 1: 0.7452898550724637, Test AUC 2: 0.7337301587301588\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results_df = pd.DataFrame(columns=['# HL', '# Node', 'Activation Function', 'Dropout', 'Batch Size', 'AUC Train', 'AUC Test 1', 'AUC Test 2'])\n",
    "\n",
    "# hyperparameters\n",
    "n_layers_values = [2, 4]\n",
    "n_nodes_values = [4, 6]\n",
    "activation_functions = ['relu', 'tanh']\n",
    "dropout_rates = [0.5, 1.0]  # 50% dropout and no dropout\n",
    "batch_sizes = [100, 10000]\n",
    "epochs = 20\n",
    "\n",
    "# Loop over each number of hidden layers\n",
    "for n_layers in n_layers_values:\n",
    "    for n_node in n_nodes_values:\n",
    "        for activation in activation_functions:\n",
    "            for dropout in dropout_rates:\n",
    "                for batch_size in batch_sizes:\n",
    "                    # Build and compile the model\n",
    "                    model = tf.keras.models.Sequential()\n",
    "                    model.add(tf.keras.layers.Dense(n_node, activation=activation, input_shape=(X_train.shape[1],)))\n",
    "                    for _ in range(n_layers - 1):\n",
    "                        model.add(tf.keras.layers.Dense(n_node, activation=activation))\n",
    "                        if dropout < 1.0:\n",
    "                            model.add(tf.keras.layers.Dropout(dropout))\n",
    "                    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "                    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.AUC()])\n",
    "\n",
    "                    # Train the model\n",
    "                    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "                    # Evaluate the model\n",
    "                    auc_train = roc_auc_score(y_train, model.predict(X_train))\n",
    "                    auc_test1 = roc_auc_score(y_test1, model.predict(X_test1))\n",
    "                    auc_test2 = roc_auc_score(y_test2, model.predict(X_test2))\n",
    "\n",
    "                    # Create a DataFrame from the results\n",
    "                    result_dict = {\n",
    "                        '# HL': n_layers,\n",
    "                        '# Node': n_node,\n",
    "                        'Activation Function': activation,\n",
    "                        'Dropout': f\"{int((1 - dropout) * 100)}%\",\n",
    "                        'Batch Size': batch_size,\n",
    "                        'AUC Train': auc_train,\n",
    "                        'AUC Test 1': auc_test1,\n",
    "                        'AUC Test 2': auc_test2\n",
    "                    }\n",
    "\n",
    "                    result_df = pd.DataFrame([result_dict])\n",
    "\n",
    "                    # Concatenate the DataFrame to results_df\n",
    "                    results_df = pd.concat([results_df, result_df], ignore_index=True)\n",
    "\n",
    "                    # Save the results after each iteration\n",
    "                    results_df.to_csv('grid_search_results.csv', index=False)\n",
    "\n",
    "                    print(f\"Model with {n_layers} layers, {n_node} nodes per layer, {activation} activation, \"\n",
    "                          f\"{int((1 - dropout) * 100)}% dropout, and batch size {batch_size} finished training. \"\n",
    "                          f\"Train AUC: {auc_train}, Test AUC 1: {auc_test1}, Test AUC 2: {auc_test2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># HL</th>\n",
       "      <th># Node</th>\n",
       "      <th>Activation Function</th>\n",
       "      <th>Dropout</th>\n",
       "      <th>Batch Size</th>\n",
       "      <th>AUC Train</th>\n",
       "      <th>AUC Test 1</th>\n",
       "      <th>AUC Test 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>relu</td>\n",
       "      <td>50%</td>\n",
       "      <td>100</td>\n",
       "      <td>0.897852</td>\n",
       "      <td>0.872826</td>\n",
       "      <td>0.911508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>relu</td>\n",
       "      <td>50%</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.680541</td>\n",
       "      <td>0.412862</td>\n",
       "      <td>0.291071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>relu</td>\n",
       "      <td>0%</td>\n",
       "      <td>100</td>\n",
       "      <td>0.932935</td>\n",
       "      <td>0.869203</td>\n",
       "      <td>0.900198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>relu</td>\n",
       "      <td>0%</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.779221</td>\n",
       "      <td>0.520471</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>50%</td>\n",
       "      <td>100</td>\n",
       "      <td>0.934122</td>\n",
       "      <td>0.887862</td>\n",
       "      <td>0.895833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>50%</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.775422</td>\n",
       "      <td>0.753986</td>\n",
       "      <td>0.732143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0%</td>\n",
       "      <td>100</td>\n",
       "      <td>0.943138</td>\n",
       "      <td>0.877899</td>\n",
       "      <td>0.911310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0%</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.591719</td>\n",
       "      <td>0.621196</td>\n",
       "      <td>0.641468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>relu</td>\n",
       "      <td>50%</td>\n",
       "      <td>100</td>\n",
       "      <td>0.917966</td>\n",
       "      <td>0.855616</td>\n",
       "      <td>0.886905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>relu</td>\n",
       "      <td>50%</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.811939</td>\n",
       "      <td>0.702899</td>\n",
       "      <td>0.789484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>relu</td>\n",
       "      <td>0%</td>\n",
       "      <td>100</td>\n",
       "      <td>0.930933</td>\n",
       "      <td>0.862138</td>\n",
       "      <td>0.909226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>relu</td>\n",
       "      <td>0%</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.803649</td>\n",
       "      <td>0.693478</td>\n",
       "      <td>0.752976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>tanh</td>\n",
       "      <td>50%</td>\n",
       "      <td>100</td>\n",
       "      <td>0.940481</td>\n",
       "      <td>0.868116</td>\n",
       "      <td>0.909524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>tanh</td>\n",
       "      <td>50%</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.748842</td>\n",
       "      <td>0.778442</td>\n",
       "      <td>0.808730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0%</td>\n",
       "      <td>100</td>\n",
       "      <td>0.939684</td>\n",
       "      <td>0.895290</td>\n",
       "      <td>0.923611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0%</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.784190</td>\n",
       "      <td>0.796196</td>\n",
       "      <td>0.778770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>relu</td>\n",
       "      <td>50%</td>\n",
       "      <td>100</td>\n",
       "      <td>0.705354</td>\n",
       "      <td>0.779710</td>\n",
       "      <td>0.815575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>relu</td>\n",
       "      <td>50%</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.496971</td>\n",
       "      <td>0.579710</td>\n",
       "      <td>0.611706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>relu</td>\n",
       "      <td>0%</td>\n",
       "      <td>100</td>\n",
       "      <td>0.916842</td>\n",
       "      <td>0.878261</td>\n",
       "      <td>0.888492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>relu</td>\n",
       "      <td>0%</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.452473</td>\n",
       "      <td>0.374728</td>\n",
       "      <td>0.472619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>50%</td>\n",
       "      <td>100</td>\n",
       "      <td>0.901811</td>\n",
       "      <td>0.880797</td>\n",
       "      <td>0.886905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>50%</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.640530</td>\n",
       "      <td>0.698188</td>\n",
       "      <td>0.642659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0%</td>\n",
       "      <td>100</td>\n",
       "      <td>0.941588</td>\n",
       "      <td>0.857609</td>\n",
       "      <td>0.909325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0%</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.821673</td>\n",
       "      <td>0.823551</td>\n",
       "      <td>0.899405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>relu</td>\n",
       "      <td>50%</td>\n",
       "      <td>100</td>\n",
       "      <td>0.915659</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.890179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>relu</td>\n",
       "      <td>50%</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.577415</td>\n",
       "      <td>0.462862</td>\n",
       "      <td>0.404762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>relu</td>\n",
       "      <td>0%</td>\n",
       "      <td>100</td>\n",
       "      <td>0.940339</td>\n",
       "      <td>0.848732</td>\n",
       "      <td>0.886310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>relu</td>\n",
       "      <td>0%</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.799495</td>\n",
       "      <td>0.765036</td>\n",
       "      <td>0.731746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>tanh</td>\n",
       "      <td>50%</td>\n",
       "      <td>100</td>\n",
       "      <td>0.910332</td>\n",
       "      <td>0.804891</td>\n",
       "      <td>0.896627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>tanh</td>\n",
       "      <td>50%</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.786706</td>\n",
       "      <td>0.737681</td>\n",
       "      <td>0.796627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0%</td>\n",
       "      <td>100</td>\n",
       "      <td>0.950454</td>\n",
       "      <td>0.882246</td>\n",
       "      <td>0.910913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0%</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.766228</td>\n",
       "      <td>0.745290</td>\n",
       "      <td>0.733730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   # HL # Node Activation Function Dropout Batch Size  AUC Train  AUC Test 1  \\\n",
       "0     2      4                relu     50%        100   0.897852    0.872826   \n",
       "1     2      4                relu     50%      10000   0.680541    0.412862   \n",
       "2     2      4                relu      0%        100   0.932935    0.869203   \n",
       "3     2      4                relu      0%      10000   0.779221    0.520471   \n",
       "4     2      4                tanh     50%        100   0.934122    0.887862   \n",
       "5     2      4                tanh     50%      10000   0.775422    0.753986   \n",
       "6     2      4                tanh      0%        100   0.943138    0.877899   \n",
       "7     2      4                tanh      0%      10000   0.591719    0.621196   \n",
       "8     2      6                relu     50%        100   0.917966    0.855616   \n",
       "9     2      6                relu     50%      10000   0.811939    0.702899   \n",
       "10    2      6                relu      0%        100   0.930933    0.862138   \n",
       "11    2      6                relu      0%      10000   0.803649    0.693478   \n",
       "12    2      6                tanh     50%        100   0.940481    0.868116   \n",
       "13    2      6                tanh     50%      10000   0.748842    0.778442   \n",
       "14    2      6                tanh      0%        100   0.939684    0.895290   \n",
       "15    2      6                tanh      0%      10000   0.784190    0.796196   \n",
       "16    4      4                relu     50%        100   0.705354    0.779710   \n",
       "17    4      4                relu     50%      10000   0.496971    0.579710   \n",
       "18    4      4                relu      0%        100   0.916842    0.878261   \n",
       "19    4      4                relu      0%      10000   0.452473    0.374728   \n",
       "20    4      4                tanh     50%        100   0.901811    0.880797   \n",
       "21    4      4                tanh     50%      10000   0.640530    0.698188   \n",
       "22    4      4                tanh      0%        100   0.941588    0.857609   \n",
       "23    4      4                tanh      0%      10000   0.821673    0.823551   \n",
       "24    4      6                relu     50%        100   0.915659    0.854167   \n",
       "25    4      6                relu     50%      10000   0.577415    0.462862   \n",
       "26    4      6                relu      0%        100   0.940339    0.848732   \n",
       "27    4      6                relu      0%      10000   0.799495    0.765036   \n",
       "28    4      6                tanh     50%        100   0.910332    0.804891   \n",
       "29    4      6                tanh     50%      10000   0.786706    0.737681   \n",
       "30    4      6                tanh      0%        100   0.950454    0.882246   \n",
       "31    4      6                tanh      0%      10000   0.766228    0.745290   \n",
       "\n",
       "    AUC Test 2  \n",
       "0     0.911508  \n",
       "1     0.291071  \n",
       "2     0.900198  \n",
       "3     0.533333  \n",
       "4     0.895833  \n",
       "5     0.732143  \n",
       "6     0.911310  \n",
       "7     0.641468  \n",
       "8     0.886905  \n",
       "9     0.789484  \n",
       "10    0.909226  \n",
       "11    0.752976  \n",
       "12    0.909524  \n",
       "13    0.808730  \n",
       "14    0.923611  \n",
       "15    0.778770  \n",
       "16    0.815575  \n",
       "17    0.611706  \n",
       "18    0.888492  \n",
       "19    0.472619  \n",
       "20    0.886905  \n",
       "21    0.642659  \n",
       "22    0.909325  \n",
       "23    0.899405  \n",
       "24    0.890179  \n",
       "25    0.404762  \n",
       "26    0.886310  \n",
       "27    0.731746  \n",
       "28    0.896627  \n",
       "29    0.796627  \n",
       "30    0.910913  \n",
       "31    0.733730  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Identify the best model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# HL                          2\n",
       "# Node                        6\n",
       "Activation Function        tanh\n",
       "Dropout                      0%\n",
       "Batch Size                  100\n",
       "AUC Train              0.939684\n",
       "AUC Test 1              0.89529\n",
       "AUC Test 2             0.923611\n",
       "Name: 14, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# highest AUC Test score\n",
    "best_model_select = results_df.loc[results_df['AUC Test 1'].idxmax()]\n",
    "best_model_select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*parameters of the best model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters of the best model\n",
    "best_n_layers = int(best_model_select['# HL'])\n",
    "best_n_nodes = int(best_model_select['# Node'])\n",
    "best_activation = best_model_select['Activation Function']\n",
    "best_dropout = float(best_model_select['Dropout'].strip('%')) / 100.0\n",
    "best_batch_size = int(best_model_select['Batch Size'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*re-run the model with optimum parameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "8/8 [==============================] - 1s 2ms/step - loss: 0.6277 - auc: 0.7339\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5693 - auc: 0.8043\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5282 - auc: 0.8405\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4965 - auc: 0.8648\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4742 - auc: 0.8782\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4556 - auc: 0.8885\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4401 - auc: 0.8970\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4266 - auc: 0.9033\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4142 - auc: 0.9090\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4022 - auc: 0.9137\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3915 - auc: 0.9169\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3813 - auc: 0.9202\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3712 - auc: 0.9229\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3614 - auc: 0.9262\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3522 - auc: 0.9294\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3432 - auc: 0.9314\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3342 - auc: 0.9333\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3258 - auc: 0.9362\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3172 - auc: 0.9388\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3096 - auc: 0.9406\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# best model with optimum parameters\n",
    "final_nn_model = Sequential()\n",
    "final_nn_model.add(Dense(best_n_nodes, activation=best_activation, input_shape=(X_train.shape[1],)))\n",
    "for _ in range(best_n_layers - 1):\n",
    "    final_nn_model.add(Dense(best_n_nodes, activation=best_activation))\n",
    "    if best_dropout < 1.0:\n",
    "        final_nn_model.add(Dropout(best_dropout))\n",
    "final_nn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the best model\n",
    "final_nn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['AUC'])\n",
    "\n",
    "# Train the best model on the entire training set\n",
    "final_nn_model.fit(X_train, y_train, epochs=20, batch_size=best_batch_size, verbose=1)\n",
    "\n",
    "final_nn_model.save('final_nn_model.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose the best model among NN and XGB**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Method 1 - compare performance scores of both best models*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg AUC value of XGB Model : 0.9169082125603865\n",
      "Avg AUC value of NN Model : 0.9094504830917873\n",
      "The best model is XGB Model with Avg AUC value of : 0.91691\n"
     ]
    }
   ],
   "source": [
    "# AUC scores from best_model_row\n",
    "auc_test1_model_row = best_model_row['AUC Test 1']\n",
    "auc_test2_model_row = best_model_row['AUC Test 2']\n",
    "\n",
    "# average AUC score for best_model_row\n",
    "best_model_row_avg_auc = (auc_test1_model_row + auc_test2_model_row) / 2\n",
    "print(\"Avg AUC value of XGB Model :\",best_model_row_avg_auc)\n",
    "\n",
    "# AUC scores from best_model_select\n",
    "auc_test1_model_select = best_model_select['AUC Test 1']\n",
    "auc_test2_model_select = best_model_select['AUC Test 2']\n",
    "\n",
    "# average AUC score for best_model_select\n",
    "best_model_select_avg_auc = (auc_test1_model_select + auc_test2_model_select) / 2\n",
    "print(\"Avg AUC value of NN Model :\",best_model_select_avg_auc)\n",
    "\n",
    "# Determine the best model and label the AUC values\n",
    "if best_model_row_avg_auc > best_model_select_avg_auc:\n",
    "    best_model = \"XGB Model\"  # XGBoost model is better\n",
    "    best_avg_auc = best_model_row_avg_auc\n",
    "    model_label = \"Avg AUC value of \"\n",
    "else:\n",
    "    best_model = \"NN Model\"  # Neural Network model is better\n",
    "    best_avg_auc = best_model_select_avg_auc\n",
    "    model_label = \"Avg AUC value of \"\n",
    "\n",
    "\n",
    "print(f\"The best model is {best_model} with {model_label}: {best_avg_auc:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Method 2 : Run the model and Compare the results*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 1ms/step\n",
      "ROC AUC - NN Model: 0.8776\n",
      "ROC AUC - XGBoost Model: 0.8206\n",
      "Best Model: NN (ROC AUC = 0.8776)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# trained NN model\n",
    "final_nn_model = load_model('final_nn_model.keras')\n",
    "\n",
    "# trained XGBoost model\n",
    "final_xgb_model = xgb.Booster(model_file='xgb_best_model.json')\n",
    "\n",
    "# Make predictions using the NN model\n",
    "nn_predictions = final_nn_model.predict(X_test)\n",
    "nn_auc = roc_auc_score(y_test, nn_predictions)\n",
    "\n",
    "# Make predictions using the XGB model\n",
    "xgb_predictions = final_xgb_model.predict(xgb.DMatrix(X_test))\n",
    "xgb_auc = roc_auc_score(y_test, xgb_predictions)\n",
    "\n",
    "# Compare models based on ROC AUC\n",
    "if nn_auc > xgb_auc:\n",
    "    best_model = \"NN\"\n",
    "    best_auc = nn_auc\n",
    "else:\n",
    "    best_model = \"XGB\"\n",
    "    best_auc = xgb_auc\n",
    "\n",
    "# results\n",
    "print(f'ROC AUC - NN Model: {nn_auc:.4f}')\n",
    "print(f'ROC AUC - XGBoost Model: {xgb_auc:.4f}')\n",
    "print(f'Best Model: {best_model} (ROC AUC = {best_auc:.4f})')\n",
    "\n",
    "# Save the best model\n",
    "if best_model == \"NN\":\n",
    "    final_nn_model.save('best_model.h5')\n",
    "else:\n",
    "    final_xgb_model.save_model('xgb_best_model.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Strategy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Load the XGBoost model\n",
    "xgb_model = xgb.Booster(model_file='xgb_best_model.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Portfolios default rate - each strategy - each time frame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Rate - Conservative Strategy (Test Set 1): 0.06666666666666667\n",
      "Default Rate - Aggressive Strategy (Test Set 1): 0.07017543859649122\n",
      "Default Rate - Conservative Strategy (Test Set 2): 0.044642857142857144\n",
      "Default Rate - Aggressive Strategy (Test Set 2): 0.06611570247933884\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using the XGBoost model on X_test1\n",
    "xgb_predictions_test1 = xgb_model.predict(xgb.DMatrix(X_test1))\n",
    "\n",
    "# Make predictions using the XGBoost model on X_test2\n",
    "xgb_predictions_test2 = xgb_model.predict(xgb.DMatrix(X_test2))\n",
    "\n",
    "# Define conservative and aggressive thresholds\n",
    "conservative_threshold = 0.2\n",
    "aggressive_threshold = 0.3\n",
    "\n",
    "# Calculate the default rate for the conservative strategy on X_test1\n",
    "accepted_indices_conservative_test1 = np.where(xgb_predictions_test1 < conservative_threshold)[0]\n",
    "default_rate_conservative_test1 = np.mean(y_test1.iloc[accepted_indices_conservative_test1])\n",
    "\n",
    "# Calculate the default rate for the aggressive strategy on X_test1\n",
    "accepted_indices_aggressive_test1 = np.where(xgb_predictions_test1 < aggressive_threshold)[0]\n",
    "default_rate_aggressive_test1 = np.mean(y_test1.iloc[accepted_indices_aggressive_test1])\n",
    "\n",
    "# Calculate the default rate for the conservative strategy on X_test2\n",
    "accepted_indices_conservative_test2 = np.where(xgb_predictions_test2 < conservative_threshold)[0]\n",
    "default_rate_conservative_test2 = np.mean(y_test2.iloc[accepted_indices_conservative_test2])\n",
    "\n",
    "# Calculate the default rate for the aggressive strategy on X_test2\n",
    "accepted_indices_aggressive_test2 = np.where(xgb_predictions_test2 < aggressive_threshold)[0]\n",
    "default_rate_aggressive_test2 = np.mean(y_test2.iloc[accepted_indices_aggressive_test2])\n",
    "\n",
    "# Print the default rates for both strategies on both test sets\n",
    "print(\"Default Rate - Conservative Strategy (Test Set 1):\", default_rate_conservative_test1)\n",
    "print(\"Default Rate - Aggressive Strategy (Test Set 1):\", default_rate_aggressive_test1)\n",
    "print(\"Default Rate - Conservative Strategy (Test Set 2):\", default_rate_conservative_test2)\n",
    "print(\"Default Rate - Aggressive Strategy (Test Set 2):\", default_rate_aggressive_test2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date Range: 2017-05-01 to 2018-01-31\n",
      "Default Rate - Conservative Strategy: 0.06666666666666667\n",
      "Default Rate - Aggressive Strategy: 0.07017543859649122\n",
      "\n",
      "Date Range: 2017-03-01 to 2017-04-30\n",
      "Default Rate - Conservative Strategy: 0.06666666666666667\n",
      "Default Rate - Aggressive Strategy: 0.07017543859649122\n",
      "\n",
      "Date Range: 2018-02-01 to 2018-03-31\n",
      "Default Rate - Conservative Strategy: 0.06666666666666667\n",
      "Default Rate - Aggressive Strategy: 0.07017543859649122\n",
      "\n",
      "Date Range: 2017-05-01 to 2018-03-31\n",
      "Default Rate - Conservative Strategy: 0.06666666666666667\n",
      "Default Rate - Aggressive Strategy: 0.07017543859649122\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the 'S_2' column contains date values\n",
    "df2['S_2'] = pd.to_datetime(df2['S_2'])\n",
    "\n",
    "# Define the date ranges\n",
    "date_ranges = [\n",
    "    ('2017-05-01', '2018-01-31'),  # May '17 to Jan '18\n",
    "    ('2017-03-01', '2017-04-30'),  # Mar '17 to Apr '17\n",
    "    ('2018-02-01', '2018-03-31'),  # Feb '18 to Mar '18\n",
    "    ('2017-05-01', '2018-03-31')   # May '17 to Mar '18\n",
    "]\n",
    "\n",
    "# Define the conservative and aggressive thresholds\n",
    "conservative_threshold = 0.2\n",
    "aggressive_threshold = 0.3\n",
    "\n",
    "# Create a dictionary to store the default rates for each date range and strategy\n",
    "default_rates_by_date_range = {}\n",
    "\n",
    "# Iterate through the date ranges and calculate default rates for each strategy\n",
    "for start_date_str, end_date_str in date_ranges:\n",
    "    start_date = pd.to_datetime(start_date_str)\n",
    "    end_date = pd.to_datetime(end_date_str)\n",
    "    \n",
    "    # Filter the DataFrame for the current date range\n",
    "    date_range_df = df2[(df2['S_2'] >= start_date) & (df2['S_2'] <= end_date)]\n",
    "    \n",
    "    # Calculate the accepted indices based on your conservative and aggressive strategies within the date range\n",
    "    accepted_indices_conservative = np.where(xgb_predictions_test1 < conservative_threshold)[0]\n",
    "    accepted_indices_aggressive = np.where(xgb_predictions_test1 < aggressive_threshold)[0]\n",
    "    \n",
    "    # Calculate the default rate for the conservative strategy within the date range\n",
    "    default_rate_conservative = np.mean(y_test1.iloc[accepted_indices_conservative])\n",
    "    \n",
    "    # Calculate the default rate for the aggressive strategy within the date range\n",
    "    default_rate_aggressive = np.mean(y_test1.iloc[accepted_indices_aggressive])\n",
    "    \n",
    "    # Store the results in the dictionary\n",
    "    default_rates_by_date_range[(start_date_str, end_date_str)] = {\n",
    "        'Conservative': default_rate_conservative,\n",
    "        'Aggressive': default_rate_aggressive\n",
    "    }\n",
    "\n",
    "# Print the default rates for each date range and strategy\n",
    "for date_range, rates in default_rates_by_date_range.items():\n",
    "    start_date_str, end_date_str = date_range\n",
    "    print(f\"Date Range: {start_date_str} to {end_date_str}\")\n",
    "    print(\"Default Rate - Conservative Strategy:\", rates['Conservative'])\n",
    "    print(\"Default Rate - Aggressive Strategy:\", rates['Aggressive'])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Portfolios expected revenue - each strategy - each time frame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named 'df'\n",
    "# Filter columns that start with \"B_\"\n",
    "balance_columns = [col for col in df2.columns if col.startswith('B_')]\n",
    "\n",
    "# Calculate B_Ave for each row (observation)\n",
    "df2['B_Ave'] = df2[balance_columns].mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\itsmv\\AppData\\Local\\Temp\\ipykernel_3512\\121566481.py:6: FutureWarning: DataFrame.mean and DataFrame.median with numeric_only=None will include datetime64 and datetime64tz columns in a future version.\n",
      "  df2['S_Ave'] = df2[spend_columns].mean(axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Assuming your DataFrame is named 'df'\n",
    "# Filter columns that start with \"S_\"\n",
    "spend_columns = [col for col in df2.columns if col.startswith('S_')]\n",
    "\n",
    "# Calculate S_Ave for each row (observation)\n",
    "df2['S_Ave'] = df2[spend_columns].mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predictions for the applicants using your XGBoost model\n",
    "xgb_predictions = xgb_model.predict(xgb.DMatrix(X_test))  # Replace X_test with your test data\n",
    "\n",
    "# Define conservative and aggressive thresholds\n",
    "conservative_threshold = 0.2\n",
    "aggressive_threshold = 0.3\n",
    "\n",
    "# Determine accepted_indices based on the strategy\n",
    "accepted_indices_conservative = np.where(xgb_predictions < conservative_threshold)[0]\n",
    "accepted_indices_aggressive = np.where(xgb_predictions < aggressive_threshold)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portfolio's Expected Revenue (Conservative Strategy): 6.110553554689037\n",
      "Portfolio's Expected Revenue (Aggressive Strategy): 12.135803022962513\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have already selected your spend and balance features and calculated their averages\n",
    "# Replace 'S_Ave' and 'B_Ave' with the actual column names representing the average spend and balance\n",
    "\n",
    "# Calculate monthly revenue for each customer\n",
    "df2['Monthly_Revenue'] = df2['B_Ave'] * 0.02 + df2['S_Ave'] * 0.001\n",
    "\n",
    "# Estimate expected revenue for the next 12 months\n",
    "df2['Expected_Revenue_12_Months'] = df2['Monthly_Revenue'] * 12\n",
    "\n",
    "# Define the accepted indices based on your conservative and aggressive strategies\n",
    "accepted_indices_conservative = np.where(xgb_predictions < conservative_threshold)[0]\n",
    "accepted_indices_aggressive = np.where(xgb_predictions < aggressive_threshold)[0]\n",
    "\n",
    "# Calculate the portfolio's expected revenue for each strategy\n",
    "portfolio_expected_revenue_conservative = df2.loc[accepted_indices_conservative, 'Expected_Revenue_12_Months'].sum()\n",
    "portfolio_expected_revenue_aggressive = df2.loc[accepted_indices_aggressive, 'Expected_Revenue_12_Months'].sum()\n",
    "\n",
    "# Print the portfolio's expected revenue for both strategies\n",
    "print(\"Portfolio's Expected Revenue (Conservative Strategy):\", portfolio_expected_revenue_conservative)\n",
    "print(\"Portfolio's Expected Revenue (Aggressive Strategy):\", portfolio_expected_revenue_aggressive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date Range: 2017-05-01 to 2018-01-31\n",
      "Portfolio's Expected Revenue (Conservative Strategy): 4.453564850018362\n",
      "Portfolio's Expected Revenue (Aggressive Strategy): 8.460670364630644\n",
      "\n",
      "Date Range: 2017-03-01 to 2017-04-30\n",
      "Portfolio's Expected Revenue (Conservative Strategy): 0.8621960086156295\n",
      "Portfolio's Expected Revenue (Aggressive Strategy): 1.7028436793853363\n",
      "\n",
      "Date Range: 2018-02-01 to 2018-03-31\n",
      "Portfolio's Expected Revenue (Conservative Strategy): 0.7947926960550442\n",
      "Portfolio's Expected Revenue (Aggressive Strategy): 1.9722889789465308\n",
      "\n",
      "Date Range: 2017-05-01 to 2018-03-31\n",
      "Portfolio's Expected Revenue (Conservative Strategy): 5.248357546073407\n",
      "Portfolio's Expected Revenue (Aggressive Strategy): 10.432959343577174\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the 'S_2' column contains date values\n",
    "df2['S_2'] = pd.to_datetime(df2['S_2'])\n",
    "\n",
    "# Define the date ranges\n",
    "date_ranges = [\n",
    "    ('2017-05-01', '2018-01-31'),  # May '17 to Jan '18\n",
    "    ('2017-03-01', '2017-04-30'),  # Mar '17 to Apr '17\n",
    "    ('2018-02-01', '2018-03-31'),  # Feb '18 to Mar '18\n",
    "    ('2017-05-01', '2018-03-31')   # May '17 to Mar '18\n",
    "]\n",
    "\n",
    "# Define the accepted indices based on your conservative and aggressive strategies\n",
    "accepted_indices_conservative = np.where(xgb_predictions < conservative_threshold)[0]\n",
    "accepted_indices_aggressive = np.where(xgb_predictions < aggressive_threshold)[0]\n",
    "\n",
    "# Create a dictionary to store the expected revenue for each date range and strategy\n",
    "expected_revenue_by_date_range = {}\n",
    "\n",
    "# Iterate through the date ranges and calculate expected revenue for each strategy\n",
    "for start_date_str, end_date_str in date_ranges:\n",
    "    start_date = pd.to_datetime(start_date_str)\n",
    "    end_date = pd.to_datetime(end_date_str)\n",
    "    \n",
    "    # Filter the DataFrame for the current date range\n",
    "    date_range_df = df2[(df2['S_2'] >= start_date) & (df2['S_2'] <= end_date)]\n",
    "    \n",
    "    # Calculate the portfolio's expected revenue for both strategies within the date range\n",
    "    portfolio_expected_revenue_conservative = date_range_df.loc[date_range_df.index.isin(accepted_indices_conservative), 'Expected_Revenue_12_Months'].sum()\n",
    "    portfolio_expected_revenue_aggressive = date_range_df.loc[date_range_df.index.isin(accepted_indices_aggressive), 'Expected_Revenue_12_Months'].sum()\n",
    "    \n",
    "    # Store the results in the dictionary\n",
    "    expected_revenue_by_date_range[(start_date_str, end_date_str)] = {\n",
    "        'Conservative': portfolio_expected_revenue_conservative,\n",
    "        'Aggressive': portfolio_expected_revenue_aggressive\n",
    "    }\n",
    "\n",
    "# Print the expected revenue for each date range and strategy\n",
    "for date_range, revenue in expected_revenue_by_date_range.items():\n",
    "    start_date_str, end_date_str = date_range\n",
    "    print(f\"Date Range: {start_date_str} to {end_date_str}\")\n",
    "    print(\"Portfolio's Expected Revenue (Conservative Strategy):\", revenue['Conservative'])\n",
    "    print(\"Portfolio's Expected Revenue (Aggressive Strategy):\", revenue['Aggressive'])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of observations - each strategy - each time frame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date Range: 2017-05-01 to 2018-01-31\n",
      "Number of observations in the Conservative Strategy: 83\n",
      "Number of observations in the Aggressive Strategy: 159\n",
      "\n",
      "Date Range: 2017-03-01 to 2017-04-30\n",
      "Number of observations in the Conservative Strategy: 16\n",
      "Number of observations in the Aggressive Strategy: 33\n",
      "\n",
      "Date Range: 2018-02-01 to 2018-03-31\n",
      "Number of observations in the Conservative Strategy: 16\n",
      "Number of observations in the Aggressive Strategy: 37\n",
      "\n",
      "Date Range: 2017-05-01 to 2018-03-31\n",
      "Number of observations in the Conservative Strategy: 99\n",
      "Number of observations in the Aggressive Strategy: 196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the 'S_2' column contains date values\n",
    "df2['S_2'] = pd.to_datetime(df2['S_2'])\n",
    "\n",
    "# Define the date ranges\n",
    "date_ranges = [\n",
    "    ('2017-05-01', '2018-01-31'),  # May '17 to Jan '18\n",
    "    ('2017-03-01', '2017-04-30'),  # Mar '17 to Apr '17\n",
    "    ('2018-02-01', '2018-03-31'),  # Feb '18 to Mar '18\n",
    "    ('2017-05-01', '2018-03-31')   # May '17 to Mar '18\n",
    "]\n",
    "\n",
    "# Create an empty dictionary to store the counts for each date range\n",
    "counts_by_date_range = {}\n",
    "\n",
    "# Iterate through the date ranges and calculate counts for each\n",
    "for start_date_str, end_date_str in date_ranges:\n",
    "    start_date = pd.to_datetime(start_date_str)\n",
    "    end_date = pd.to_datetime(end_date_str)\n",
    "    \n",
    "    # Filter the DataFrame for the current date range\n",
    "    date_range_df = df2[(df2['S_2'] >= start_date) & (df2['S_2'] <= end_date)]\n",
    "    \n",
    "    # Count the number of observations in the conservative and aggressive strategies within the date range\n",
    "    count_conservative = len(date_range_df.index.intersection(accepted_indices_conservative))\n",
    "    count_aggressive = len(date_range_df.index.intersection(accepted_indices_aggressive))\n",
    "    \n",
    "    # Store the counts in the dictionary\n",
    "    counts_by_date_range[(start_date_str, end_date_str)] = {\n",
    "        'Conservative': count_conservative,\n",
    "        'Aggressive': count_aggressive\n",
    "    }\n",
    "\n",
    "# Print the counts for each date range\n",
    "for date_range, counts in counts_by_date_range.items():\n",
    "    start_date_str, end_date_str = date_range\n",
    "    print(f\"Date Range: {start_date_str} to {end_date_str}\")\n",
    "    print(\"Number of observations in the Conservative Strategy:\", counts['Conservative'])\n",
    "    print(\"Number of observations in the Aggressive Strategy:\", counts['Aggressive'])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of months of data available for customers and their default rates** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of customers with 1 month(s) of data: 105\n",
      "Default rate for customers with 1 month(s) of data: 21.90%\n",
      "\n",
      "Number of customers with 2 month(s) of data: 87\n",
      "Default rate for customers with 2 month(s) of data: 26.44%\n",
      "\n",
      "Number of customers with 3 month(s) of data: 85\n",
      "Default rate for customers with 3 month(s) of data: 25.88%\n",
      "\n",
      "Number of customers with 4 month(s) of data: 81\n",
      "Default rate for customers with 4 month(s) of data: 27.16%\n",
      "\n",
      "Number of customers with 5 month(s) of data: 108\n",
      "Default rate for customers with 5 month(s) of data: 31.48%\n",
      "\n",
      "Number of customers with 6 month(s) of data: 78\n",
      "Default rate for customers with 6 month(s) of data: 30.77%\n",
      "\n",
      "Number of customers with 7 month(s) of data: 96\n",
      "Default rate for customers with 7 month(s) of data: 28.12%\n",
      "\n",
      "Number of customers with 8 month(s) of data: 73\n",
      "Default rate for customers with 8 month(s) of data: 26.03%\n",
      "\n",
      "Number of customers with 9 month(s) of data: 88\n",
      "Default rate for customers with 9 month(s) of data: 23.86%\n",
      "\n",
      "Number of customers with 10 month(s) of data: 101\n",
      "Default rate for customers with 10 month(s) of data: 24.75%\n",
      "\n",
      "Number of customers with 11 month(s) of data: 74\n",
      "Default rate for customers with 11 month(s) of data: 21.62%\n",
      "\n",
      "Number of customers with 12 month(s) of data: 91\n",
      "Default rate for customers with 12 month(s) of data: 20.88%\n",
      "\n",
      "Number of customers with 13 month(s) of data: 66\n",
      "Default rate for customers with 13 month(s) of data: 22.73%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the 'S_2' column contains date values\n",
    "df2['S_2'] = pd.to_datetime(df2['S_2'])\n",
    "\n",
    "# Define the start and end date for the analysis\n",
    "end_date = pd.to_datetime('2018-03-31')\n",
    "\n",
    "# Initialize dictionaries to store the counts and default rates for each month\n",
    "counts_by_month = {}\n",
    "default_rates_by_month = {}\n",
    "\n",
    "# Iterate through each month from March 2018 to March 2017\n",
    "current_date = end_date\n",
    "month_number = 1  # Initialize the month number\n",
    "\n",
    "while current_date >= pd.to_datetime('2017-03-01'):\n",
    "    # Calculate the start date for the current month\n",
    "    start_date = current_date - pd.DateOffset(months=1)\n",
    "    \n",
    "    # Filter the DataFrame for customers with records in the current month\n",
    "    customers_for_month = df2[(df2['S_2'] >= start_date) & (df2['S_2'] <= current_date)]\n",
    "    \n",
    "    # Get the unique customer IDs for the current month\n",
    "    unique_customer_ids_month = customers_for_month['customer_ID'].unique()\n",
    "    \n",
    "    # Filter the DataFrame for records before the current month\n",
    "    records_before_month = df2[df2['S_2'] < start_date]\n",
    "    \n",
    "    # Get the unique customer IDs for records before the current month\n",
    "    unique_customer_ids_before_month = records_before_month['customer_ID'].unique()\n",
    "    \n",
    "    # Find customer IDs unique to the current month (not in previous months)\n",
    "    unique_customer_ids_only_in_month = set(unique_customer_ids_month) - set(unique_customer_ids_before_month)\n",
    "    \n",
    "    # Calculate the default rate for the current set of customers\n",
    "    # You need to replace 'y' with your target variable column name\n",
    "    default_rate = customers_for_month[customers_for_month['customer_ID'].isin(unique_customer_ids_only_in_month)]['target'].mean()\n",
    "    \n",
    "    # Calculate the number of unique customers for the current month\n",
    "    month_label = current_date.strftime(\"%B %Y\")\n",
    "    counts_by_month[month_label] = len(unique_customer_ids_only_in_month)\n",
    "    \n",
    "    # Store the default rate for the current month\n",
    "    default_rates_by_month[month_label] = default_rate\n",
    "    \n",
    "    # Print the output for this month\n",
    "    print(f\"Number of customers with {month_number} month(s) of data: {counts_by_month[month_label]}\")\n",
    "    print(f\"Default rate for customers with {month_number} month(s) of data: {default_rates_by_month[month_label]:.2%}\")  # Format as a percentage\n",
    "    print()\n",
    "    \n",
    "    # Increment the month number\n",
    "    month_number += 1\n",
    "    \n",
    "    # Move to the previous month\n",
    "    current_date -= pd.DateOffset(months=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
